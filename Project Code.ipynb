{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the U.S. HDMA (Home Mortgage Disclosure Act), 2017 I am attempting to predict what loan status (Approved, denied by a lender, closed for incompleteness) an individualâ€™s application would get. An individual could benefit by using this model before going to a lender to see if they will be approved. Lenders could use this model as a preliminary check to see if they should approve or deny a loan. With the given dataset I believe a data analytics approach is appropriate because this is a prediction problem that can be solved using details about the customer and lender.\n",
    "\n",
    "For more detailed dataset information visit this link: https://www.consumerfinance.gov/data-research/hmda/historic-data/?geo=nationwide&records=all-records&field_descriptions=labels\n",
    "\n",
    "Direct Dataset Link: https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_ca_all-records_labels.zip \n",
    "\n",
    "\n",
    "Note: Running the grid search and model training steps takes around 15 hours (on my machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING Python 3.8.5\n",
    "#pipreqs used to genereate requirments.txt\n",
    "\n",
    "#Date parsing,cleaning,encoding helperes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer,FunctionTransformer, MinMaxScaler\n",
    "from sklearn_pandas import DataFrameMapper #pip install sklearn-pandas\n",
    "from sklearn import preprocessing\n",
    "import pickle as pkl\n",
    "\n",
    "#Data reduction models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Model Results/gridsearch\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limited ram and computational power, I limited my dataset down to the state of California. This new California subset has 1,714,264 datapoints and from here on out this will be referred to as the dataset. \n",
    "\n",
    "Unzip the hmda_2017_ca_all-records_labels.zip and save the csv in the same folder as this python file.\n",
    "\n",
    "Note: Alternative you can download the data direclty from: https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_ca_all-records_labels.zip \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (34,36,38,42,44,46,48) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"hmda_2017_nationwide_all-records_labels.csv\").rename_axis('ID_OG').reset_index()\n",
    "df_raw = pd.read_csv(\"hmda_2017_ca_all-records_labels.csv\").rename_axis('ID_OG').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that I won't be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns that I will be using.\n",
    "metadata_cols = ['ID_OG','respondent_id']\n",
    "target_reason_cols = ['denial_reason_name_1','denial_reason_name_2','denial_reason_name_3']\n",
    "target_col = 'action_taken_name'\n",
    "keep_cols = ['agency_abbr','loan_type_name','property_type_name','loan_purpose_name','owner_occupancy_name','loan_amount_000s','preapproval_name','msamd_name','county_name','census_tract_number','applicant_ethnicity_name',\n",
    "            'co_applicant_ethnicity_name','applicant_race_name_1', 'applicant_race_name_2','applicant_race_name_3','applicant_race_name_4','co_applicant_race_name_2','co_applicant_race_name_3','co_applicant_race_name_4',\n",
    "            'co_applicant_race_name_5', 'applicant_sex_name','co_applicant_sex_name','applicant_income_000s','purchaser_type_name','rate_spread','hoepa_status_name','lien_status_name','population','minority_population',\n",
    "            'hud_median_family_income', 'tract_to_msamd_income', 'number_of_owner_occupied_units','number_of_1_to_4_family_units']\n",
    "\n",
    "df_pre = df_raw[metadata_cols + target_reason_cols + keep_cols + [target_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am only prediciting Aprroved, denined and file incomplete for indivudals. Drop rows that contain other status (ex. preapproval Approved) or rows where loan was for another insitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_taken_name\n",
       "Application approved but not accepted           53535\n",
       "Application denied by financial institution    226258\n",
       "File closed for incompleteness                  81948\n",
       "Loan originated                                877753\n",
       "Name: agency_abbr, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_status = ['Application denied by financial institution',\n",
    "       'Loan originated','File closed for incompleteness',\n",
    "       'Application approved but not accepted']\n",
    "df_pre = df_pre[df_pre['action_taken_name'].isin(keep_status)]\n",
    "df_pre.groupby('action_taken_name').count()['agency_abbr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any rows where there is nan in a required column. The following columns are ones that I thought were required\n",
    "- loan_amount_000s: I believe the amount would have a big impact on predicting acceptance, so any rows without it are dropped\n",
    "- applicant_income_000s: I believe that applicants income also has a big impact on predicting \n",
    "- action_taken_name: this is the target column so any rows without this should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped in this step: 71726 and percentage dropped: 0.05786716192252645\n"
     ]
    }
   ],
   "source": [
    "def drop_rows_na_cols(df, non_na_cols):\n",
    "    for c in non_na_cols:\n",
    "        df = df[~df[c].isna()]\n",
    "\n",
    "    return df\n",
    "\n",
    "size_pre_drop = df_pre.shape[0]\n",
    "non_na_cols = ['loan_amount_000s','applicant_income_000s',target_col]\n",
    "df_pre = drop_rows_na_cols(df_pre,non_na_cols)\n",
    "\n",
    "print(f\"Number of rows dropped in this step: {size_pre_drop - df_pre.shape[0]} and percentage dropped: {(size_pre_drop - df_pre.shape[0])/size_pre_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_OG</th>\n",
       "      <td>1167768.0</td>\n",
       "      <td>814001.624588</td>\n",
       "      <td>480424.939231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401008.750000</td>\n",
       "      <td>800852.500000</td>\n",
       "      <td>1.203142e+06</td>\n",
       "      <td>1.711900e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <td>1167768.0</td>\n",
       "      <td>400.166048</td>\n",
       "      <td>961.813447</td>\n",
       "      <td>1.0</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>4.810000e+02</td>\n",
       "      <td>4.750000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>census_tract_number</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>1771.558287</td>\n",
       "      <td>2354.793197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.020000</td>\n",
       "      <td>426.200000</td>\n",
       "      <td>3.383020e+03</td>\n",
       "      <td>9.832000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <td>1167768.0</td>\n",
       "      <td>145.531172</td>\n",
       "      <td>250.824189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>6.078900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_spread</th>\n",
       "      <td>38655.0</td>\n",
       "      <td>2.393003</td>\n",
       "      <td>1.337057</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>1.880000</td>\n",
       "      <td>2.370000e+00</td>\n",
       "      <td>1.702000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>5856.179801</td>\n",
       "      <td>2877.458109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4108.000000</td>\n",
       "      <td>5356.000000</td>\n",
       "      <td>6.845000e+03</td>\n",
       "      <td>3.945400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minority_population</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>54.438901</td>\n",
       "      <td>24.854502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.730000</td>\n",
       "      <td>52.759998</td>\n",
       "      <td>7.588000e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hud_median_family_income</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>75246.044356</td>\n",
       "      <td>17108.911841</td>\n",
       "      <td>47300.0</td>\n",
       "      <td>63200.000000</td>\n",
       "      <td>64300.000000</td>\n",
       "      <td>8.560000e+04</td>\n",
       "      <td>1.315000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_to_msamd_income</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>118.704548</td>\n",
       "      <td>48.723378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.949997</td>\n",
       "      <td>111.709999</td>\n",
       "      <td>1.456600e+02</td>\n",
       "      <td>3.987000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_owner_occupied_units</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>1222.159539</td>\n",
       "      <td>686.812071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>1107.000000</td>\n",
       "      <td>1.529000e+03</td>\n",
       "      <td>5.229000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_1_to_4_family_units</th>\n",
       "      <td>1164349.0</td>\n",
       "      <td>1756.511056</td>\n",
       "      <td>888.761678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1598.000000</td>\n",
       "      <td>2.130000e+03</td>\n",
       "      <td>7.379000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    count           mean            std  \\\n",
       "ID_OG                           1167768.0  814001.624588  480424.939231   \n",
       "loan_amount_000s                1167768.0     400.166048     961.813447   \n",
       "census_tract_number             1164349.0    1771.558287    2354.793197   \n",
       "applicant_income_000s           1167768.0     145.531172     250.824189   \n",
       "rate_spread                       38655.0       2.393003       1.337057   \n",
       "population                      1164349.0    5856.179801    2877.458109   \n",
       "minority_population             1164349.0      54.438901      24.854502   \n",
       "hud_median_family_income        1164349.0   75246.044356   17108.911841   \n",
       "tract_to_msamd_income           1164349.0     118.704548      48.723378   \n",
       "number_of_owner_occupied_units  1164349.0    1222.159539     686.812071   \n",
       "number_of_1_to_4_family_units   1164349.0    1756.511056     888.761678   \n",
       "\n",
       "                                    min            25%            50%  \\\n",
       "ID_OG                               0.0  401008.750000  800852.500000   \n",
       "loan_amount_000s                    1.0     213.000000     333.000000   \n",
       "census_tract_number                 1.0      83.020000     426.200000   \n",
       "applicant_income_000s               1.0      68.000000     103.000000   \n",
       "rate_spread                         1.5       1.660000       1.880000   \n",
       "population                          0.0    4108.000000    5356.000000   \n",
       "minority_population                 0.0      33.730000      52.759998   \n",
       "hud_median_family_income        47300.0   63200.000000   64300.000000   \n",
       "tract_to_msamd_income               0.0      83.949997     111.709999   \n",
       "number_of_owner_occupied_units      0.0     761.000000    1107.000000   \n",
       "number_of_1_to_4_family_units       0.0    1178.000000    1598.000000   \n",
       "\n",
       "                                         75%           max  \n",
       "ID_OG                           1.203142e+06  1.711900e+06  \n",
       "loan_amount_000s                4.810000e+02  4.750000e+05  \n",
       "census_tract_number             3.383020e+03  9.832000e+03  \n",
       "applicant_income_000s           1.610000e+02  6.078900e+04  \n",
       "rate_spread                     2.370000e+00  1.702000e+01  \n",
       "population                      6.845000e+03  3.945400e+04  \n",
       "minority_population             7.588000e+01  1.000000e+02  \n",
       "hud_median_family_income        8.560000e+04  1.315000e+05  \n",
       "tract_to_msamd_income           1.456600e+02  3.987000e+02  \n",
       "number_of_owner_occupied_units  1.529000e+03  5.229000e+03  \n",
       "number_of_1_to_4_family_units   2.130000e+03  7.379000e+03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distribution of numerical columns\n",
    "df_pre.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values using the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute and wrangle the data to imputate np.nans\n",
    "    or cleaning categorical columns.\n",
    "\n",
    "    :param df: a dataframe with features\n",
    "\n",
    "    \"\"\"\n",
    "    mapper_impute = DataFrameMapper([\n",
    "        # line by line to keep column name order\n",
    "        (['ID_OG'], None),\n",
    "        (['respondent_id'], None),\n",
    "        (['denial_reason_name_1'], None),\n",
    "        (['denial_reason_name_2'], None),\n",
    "        (['denial_reason_name_3'], None),\n",
    "        (['action_taken_name'], None),\n",
    "        (['agency_abbr'], None), #Current Dataset has no missing values     SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        (['loan_type_name'], None),#Current Dataset has no missing values\n",
    "        (['property_type_name'], None),\n",
    "        (['loan_purpose_name'], None),\n",
    "        (['owner_occupancy_name'], None),\n",
    "        (['loan_amount_000s'], None),\n",
    "        (['preapproval_name'], None),\n",
    "        (['msamd_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['county_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['census_tract_number'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)),\n",
    "        (['applicant_ethnicity_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_ethnicity_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_race_name_1'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_race_name_2'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_race_name_3'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_race_name_4'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_race_name_2'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_race_name_3'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_race_name_4'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_race_name_5'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_sex_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['co_applicant_sex_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['applicant_income_000s'], SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "        (['purchaser_type_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['rate_spread'], SimpleImputer(missing_values=np.nan,  strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['hoepa_status_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['lien_status_name'], SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')),\n",
    "        (['population'], SimpleImputer(missing_values=np.nan, strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['minority_population'], SimpleImputer(missing_values=np.nan,  strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['hud_median_family_income'], SimpleImputer(missing_values=np.nan, strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['tract_to_msamd_income'], SimpleImputer(missing_values=np.nan, strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['number_of_owner_occupied_units'], SimpleImputer(missing_values=np.nan,  strategy='median')), #Other options Static (-1), mean or mode\n",
    "        (['number_of_1_to_4_family_units'], SimpleImputer(missing_values=np.nan, strategy='median')) #Other options Static (-1), mean or mode\n",
    "\n",
    "    ], df_out=True)\n",
    "\n",
    "    return mapper_impute.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_data(df_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the effects on the mean/distribution before(df_pre) and after(df_clean) filling na with median. It looks to be that for most continous columns the mean/distribution did not change by much which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_OG</th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>census_tract_number</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <th>rate_spread</th>\n",
       "      <th>population</th>\n",
       "      <th>minority_population</th>\n",
       "      <th>hud_median_family_income</th>\n",
       "      <th>tract_to_msamd_income</th>\n",
       "      <th>number_of_owner_occupied_units</th>\n",
       "      <th>number_of_1_to_4_family_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>38655.000000</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "      <td>1.164349e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.140016e+05</td>\n",
       "      <td>4.001660e+02</td>\n",
       "      <td>1.771558e+03</td>\n",
       "      <td>1.455312e+02</td>\n",
       "      <td>2.393003</td>\n",
       "      <td>5.856180e+03</td>\n",
       "      <td>5.443890e+01</td>\n",
       "      <td>7.524604e+04</td>\n",
       "      <td>1.187045e+02</td>\n",
       "      <td>1.222160e+03</td>\n",
       "      <td>1.756511e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.804249e+05</td>\n",
       "      <td>9.618134e+02</td>\n",
       "      <td>2.354793e+03</td>\n",
       "      <td>2.508242e+02</td>\n",
       "      <td>1.337057</td>\n",
       "      <td>2.877458e+03</td>\n",
       "      <td>2.485450e+01</td>\n",
       "      <td>1.710891e+04</td>\n",
       "      <td>4.872338e+01</td>\n",
       "      <td>6.868121e+02</td>\n",
       "      <td>8.887617e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.730000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.010088e+05</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>8.302000e+01</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>4.108000e+03</td>\n",
       "      <td>3.373000e+01</td>\n",
       "      <td>6.320000e+04</td>\n",
       "      <td>8.395000e+01</td>\n",
       "      <td>7.610000e+02</td>\n",
       "      <td>1.178000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.008525e+05</td>\n",
       "      <td>3.330000e+02</td>\n",
       "      <td>4.262000e+02</td>\n",
       "      <td>1.030000e+02</td>\n",
       "      <td>1.880000</td>\n",
       "      <td>5.356000e+03</td>\n",
       "      <td>5.276000e+01</td>\n",
       "      <td>6.430000e+04</td>\n",
       "      <td>1.117100e+02</td>\n",
       "      <td>1.107000e+03</td>\n",
       "      <td>1.598000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.203142e+06</td>\n",
       "      <td>4.810000e+02</td>\n",
       "      <td>3.383020e+03</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>6.845000e+03</td>\n",
       "      <td>7.588000e+01</td>\n",
       "      <td>8.560000e+04</td>\n",
       "      <td>1.456600e+02</td>\n",
       "      <td>1.529000e+03</td>\n",
       "      <td>2.130000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.711900e+06</td>\n",
       "      <td>4.750000e+05</td>\n",
       "      <td>9.832000e+03</td>\n",
       "      <td>6.078900e+04</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>3.945400e+04</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.315000e+05</td>\n",
       "      <td>3.987000e+02</td>\n",
       "      <td>5.229000e+03</td>\n",
       "      <td>7.379000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID_OG  loan_amount_000s  census_tract_number  \\\n",
       "count  1.167768e+06      1.167768e+06         1.164349e+06   \n",
       "mean   8.140016e+05      4.001660e+02         1.771558e+03   \n",
       "std    4.804249e+05      9.618134e+02         2.354793e+03   \n",
       "min    0.000000e+00      1.000000e+00         1.000000e+00   \n",
       "25%    4.010088e+05      2.130000e+02         8.302000e+01   \n",
       "50%    8.008525e+05      3.330000e+02         4.262000e+02   \n",
       "75%    1.203142e+06      4.810000e+02         3.383020e+03   \n",
       "max    1.711900e+06      4.750000e+05         9.832000e+03   \n",
       "\n",
       "       applicant_income_000s   rate_spread    population  minority_population  \\\n",
       "count           1.167768e+06  38655.000000  1.164349e+06         1.164349e+06   \n",
       "mean            1.455312e+02      2.393003  5.856180e+03         5.443890e+01   \n",
       "std             2.508242e+02      1.337057  2.877458e+03         2.485450e+01   \n",
       "min             1.000000e+00      1.500000  0.000000e+00         0.000000e+00   \n",
       "25%             6.800000e+01      1.660000  4.108000e+03         3.373000e+01   \n",
       "50%             1.030000e+02      1.880000  5.356000e+03         5.276000e+01   \n",
       "75%             1.610000e+02      2.370000  6.845000e+03         7.588000e+01   \n",
       "max             6.078900e+04     17.020000  3.945400e+04         1.000000e+02   \n",
       "\n",
       "       hud_median_family_income  tract_to_msamd_income  \\\n",
       "count              1.164349e+06           1.164349e+06   \n",
       "mean               7.524604e+04           1.187045e+02   \n",
       "std                1.710891e+04           4.872338e+01   \n",
       "min                4.730000e+04           0.000000e+00   \n",
       "25%                6.320000e+04           8.395000e+01   \n",
       "50%                6.430000e+04           1.117100e+02   \n",
       "75%                8.560000e+04           1.456600e+02   \n",
       "max                1.315000e+05           3.987000e+02   \n",
       "\n",
       "       number_of_owner_occupied_units  number_of_1_to_4_family_units  \n",
       "count                    1.164349e+06                   1.164349e+06  \n",
       "mean                     1.222160e+03                   1.756511e+03  \n",
       "std                      6.868121e+02                   8.887617e+02  \n",
       "min                      0.000000e+00                   0.000000e+00  \n",
       "25%                      7.610000e+02                   1.178000e+03  \n",
       "50%                      1.107000e+03                   1.598000e+03  \n",
       "75%                      1.529000e+03                   2.130000e+03  \n",
       "max                      5.229000e+03                   7.379000e+03  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_OG</th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>census_tract_number</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <th>rate_spread</th>\n",
       "      <th>population</th>\n",
       "      <th>minority_population</th>\n",
       "      <th>hud_median_family_income</th>\n",
       "      <th>tract_to_msamd_income</th>\n",
       "      <th>number_of_owner_occupied_units</th>\n",
       "      <th>number_of_1_to_4_family_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "      <td>1.167768e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.140016e+05</td>\n",
       "      <td>4.001660e+02</td>\n",
       "      <td>1.766369e+03</td>\n",
       "      <td>1.455312e+02</td>\n",
       "      <td>1.896981e+00</td>\n",
       "      <td>5.854715e+03</td>\n",
       "      <td>5.443399e+01</td>\n",
       "      <td>7.521400e+04</td>\n",
       "      <td>1.186841e+02</td>\n",
       "      <td>1.221822e+03</td>\n",
       "      <td>1.756047e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.804249e+05</td>\n",
       "      <td>9.618134e+02</td>\n",
       "      <td>2.353293e+03</td>\n",
       "      <td>2.508242e+02</td>\n",
       "      <td>2.599962e-01</td>\n",
       "      <td>2.873370e+03</td>\n",
       "      <td>2.481826e+01</td>\n",
       "      <td>1.709408e+04</td>\n",
       "      <td>4.865347e+01</td>\n",
       "      <td>6.858341e+02</td>\n",
       "      <td>8.875010e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.500000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.730000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.010088e+05</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>8.202000e+01</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>1.880000e+00</td>\n",
       "      <td>4.114000e+03</td>\n",
       "      <td>3.374000e+01</td>\n",
       "      <td>6.320000e+04</td>\n",
       "      <td>8.401000e+01</td>\n",
       "      <td>7.620000e+02</td>\n",
       "      <td>1.179000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.008525e+05</td>\n",
       "      <td>3.330000e+02</td>\n",
       "      <td>4.261700e+02</td>\n",
       "      <td>1.030000e+02</td>\n",
       "      <td>1.880000e+00</td>\n",
       "      <td>5.356000e+03</td>\n",
       "      <td>5.276000e+01</td>\n",
       "      <td>6.430000e+04</td>\n",
       "      <td>1.117100e+02</td>\n",
       "      <td>1.107000e+03</td>\n",
       "      <td>1.598000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.203142e+06</td>\n",
       "      <td>4.810000e+02</td>\n",
       "      <td>3.382010e+03</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>1.880000e+00</td>\n",
       "      <td>6.837000e+03</td>\n",
       "      <td>7.575000e+01</td>\n",
       "      <td>8.560000e+04</td>\n",
       "      <td>1.455800e+02</td>\n",
       "      <td>1.528000e+03</td>\n",
       "      <td>2.130000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.711900e+06</td>\n",
       "      <td>4.750000e+05</td>\n",
       "      <td>9.832000e+03</td>\n",
       "      <td>6.078900e+04</td>\n",
       "      <td>1.702000e+01</td>\n",
       "      <td>3.945400e+04</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.315000e+05</td>\n",
       "      <td>3.987000e+02</td>\n",
       "      <td>5.229000e+03</td>\n",
       "      <td>7.379000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID_OG  loan_amount_000s  census_tract_number  \\\n",
       "count  1.167768e+06      1.167768e+06         1.167768e+06   \n",
       "mean   8.140016e+05      4.001660e+02         1.766369e+03   \n",
       "std    4.804249e+05      9.618134e+02         2.353293e+03   \n",
       "min    0.000000e+00      1.000000e+00        -1.000000e+00   \n",
       "25%    4.010088e+05      2.130000e+02         8.202000e+01   \n",
       "50%    8.008525e+05      3.330000e+02         4.261700e+02   \n",
       "75%    1.203142e+06      4.810000e+02         3.382010e+03   \n",
       "max    1.711900e+06      4.750000e+05         9.832000e+03   \n",
       "\n",
       "       applicant_income_000s   rate_spread    population  minority_population  \\\n",
       "count           1.167768e+06  1.167768e+06  1.167768e+06         1.167768e+06   \n",
       "mean            1.455312e+02  1.896981e+00  5.854715e+03         5.443399e+01   \n",
       "std             2.508242e+02  2.599962e-01  2.873370e+03         2.481826e+01   \n",
       "min             1.000000e+00  1.500000e+00  0.000000e+00         0.000000e+00   \n",
       "25%             6.800000e+01  1.880000e+00  4.114000e+03         3.374000e+01   \n",
       "50%             1.030000e+02  1.880000e+00  5.356000e+03         5.276000e+01   \n",
       "75%             1.610000e+02  1.880000e+00  6.837000e+03         7.575000e+01   \n",
       "max             6.078900e+04  1.702000e+01  3.945400e+04         1.000000e+02   \n",
       "\n",
       "       hud_median_family_income  tract_to_msamd_income  \\\n",
       "count              1.167768e+06           1.167768e+06   \n",
       "mean               7.521400e+04           1.186841e+02   \n",
       "std                1.709408e+04           4.865347e+01   \n",
       "min                4.730000e+04           0.000000e+00   \n",
       "25%                6.320000e+04           8.401000e+01   \n",
       "50%                6.430000e+04           1.117100e+02   \n",
       "75%                8.560000e+04           1.455800e+02   \n",
       "max                1.315000e+05           3.987000e+02   \n",
       "\n",
       "       number_of_owner_occupied_units  number_of_1_to_4_family_units  \n",
       "count                    1.167768e+06                   1.167768e+06  \n",
       "mean                     1.221822e+03                   1.756047e+03  \n",
       "std                      6.858341e+02                   8.875010e+02  \n",
       "min                      0.000000e+00                   0.000000e+00  \n",
       "25%                      7.620000e+02                   1.179000e+03  \n",
       "50%                      1.107000e+03                   1.598000e+03  \n",
       "75%                      1.528000e+03                   2.130000e+03  \n",
       "max                      5.229000e+03                   7.379000e+03  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode and scaler the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scaler(arr_of_lst: np.array, median: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Scale skewed data to categorical bins, either:\n",
    "    (1) Greater than median or\n",
    "    (2) Less than or equal to median\n",
    "\n",
    "    :param arr_of_lst: an array of lists of continuous values, for\n",
    "    dollars, population.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.array([[0] if k[0] <= median else [1] for k in arr_of_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create one hot encode vectors for categorical columns and/or standard scale data\n",
    "    for numerical accordingly.\n",
    "\n",
    "    :param df: a dataframe of application data with imputed values\n",
    "\n",
    "    \"\"\"\n",
    "    medianStat = df[['hud_median_family_income','tract_to_msamd_income']].describe().T['50%']\n",
    "    mapper_encode = DataFrameMapper([\n",
    "\n",
    "        (['ID_OG'], None),\n",
    "        (['respondent_id'], None),\n",
    "        (['denial_reason_name_1'], None),\n",
    "        (['denial_reason_name_2'], None),\n",
    "        (['denial_reason_name_3'], None),\n",
    "        (['action_taken_name'], None),\n",
    "        (['agency_abbr'], LabelBinarizer()),\n",
    "        (['loan_type_name'], LabelBinarizer()),\n",
    "        (['property_type_name'], LabelBinarizer()),\n",
    "        (['loan_purpose_name'], LabelBinarizer()),\n",
    "        (['owner_occupancy_name'], LabelBinarizer()),\n",
    "        (['loan_amount_000s'], None),\n",
    "        (['preapproval_name'], LabelBinarizer()),\n",
    "        (['msamd_name'], LabelBinarizer()),\n",
    "        (['county_name'], LabelBinarizer()),\n",
    "        (['census_tract_number'], None),\n",
    "        (['applicant_ethnicity_name'], LabelBinarizer()),\n",
    "        (['co_applicant_ethnicity_name'], LabelBinarizer()),\n",
    "        (['applicant_race_name_1'], LabelBinarizer()),\n",
    "        (['applicant_race_name_2'], LabelBinarizer()),\n",
    "        (['applicant_race_name_3'], LabelBinarizer()),\n",
    "        (['applicant_race_name_4'], LabelBinarizer()),\n",
    "        (['co_applicant_race_name_2'], LabelBinarizer()),\n",
    "        (['co_applicant_race_name_3'], LabelBinarizer()),\n",
    "        (['co_applicant_race_name_4'], LabelBinarizer()),\n",
    "        (['co_applicant_race_name_5'], LabelBinarizer()),\n",
    "        (['applicant_sex_name'], LabelBinarizer()),\n",
    "        (['co_applicant_sex_name'], LabelBinarizer()),\n",
    "        (['applicant_income_000s'], None),\n",
    "        (['purchaser_type_name'], LabelBinarizer()),\n",
    "        (['rate_spread'], None),\n",
    "        (['hoepa_status_name'], LabelBinarizer()),\n",
    "        (['lien_status_name'], LabelBinarizer()),\n",
    "        (['population'], None),\n",
    "        (['minority_population'], None),\n",
    "        (['hud_median_family_income'], FunctionTransformer(custom_scaler,\n",
    "                                          kw_args={\"median\": medianStat['hud_median_family_income']})), #Might want to just use original data instead of a median bool\n",
    "        (['tract_to_msamd_income'], FunctionTransformer(custom_scaler,\n",
    "                                          kw_args={\"median\": medianStat['tract_to_msamd_income']})),\n",
    "        (['number_of_owner_occupied_units'], None),\n",
    "        (['number_of_1_to_4_family_units'], None)\n",
    "    ], df_out=True)\n",
    "\n",
    "    return mapper_encode.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns before encoding:39\n",
      "Number of columns after encoding:206\n"
     ]
    }
   ],
   "source": [
    "df_encoded = encode_data(df_clean)\n",
    "print(f\"Number of columns before encoding:{len(df_clean.columns)}\\nNumber of columns after encoding:{len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data Prep\n",
    "Get the train, validation and tests. As well as the X,y's for model training.\n",
    "The test set will only be used as a unseen test for when I have finished parameter tunning the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(df, scaler = True):\n",
    "    X = df.drop(metadata_cols+[target_col]+target_reason_cols,axis=1).values\n",
    "    if scaler:\n",
    "        #X = MinMaxScaler().fit_transform(X)\n",
    "        X = preprocessing.scale(X)\n",
    "\n",
    "    y = df[target_col].values\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random state set so this won't change each run, Also this split is stratified\n",
    "df_encoded_train_val, df_encoded_test = train_test_split(df_encoded, test_size=0.1, random_state=123, \n",
    "                                                         stratify=df_encoded[target_col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, y_train_val = get_x_y(df_encoded_train_val)\n",
    "X_test, y_test = get_x_y(df_encoded_test)\n",
    "\n",
    "del df_encoded_train_val #to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=25) #used to be 10\n",
    "X_train_val_small = pca.fit_transform(X_train_val)\n",
    "X_test_small = pca.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tunning\n",
    "I used grid search to find optimal parameters for each model. Due to the limitations of my hardware, I was only able to test a small subset of parameter variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(model_type = \"Decision Tree\",  params={}):\n",
    "    \"\"\" Return pointer to model with given params \n",
    "    :param model_type: Model to initialize\n",
    "    :param params: Params to initialize model with\n",
    "    \"\"\"\n",
    "    if model_type == \"Decision Tree\":\n",
    "        mdl = DecisionTreeClassifier(**params)\n",
    "    elif model_type == \"Logistic Regression\":\n",
    "        mdl = LogisticRegression(**params)\n",
    "    elif model_type == \"Naive Bayes\":\n",
    "        mdl = BernoulliNB(**params)\n",
    "    elif model_type == \"KNN\":\n",
    "        mdl = KNeighborsClassifier(**params)\n",
    "    elif model_type == \"Neural network\":\n",
    "        mdl = MLPClassifier(**params)\n",
    "    elif model_type == \"SVM\":\n",
    "        mdl = SVC(**params)\n",
    "    else:\n",
    "        raise Exception(\"Model Type not recognized. Try one of the following: ['Decision Tree','Logistic Regression','Naive Bayes','KNN','Neural network','SVM']\")\n",
    "\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_w_grid_search(mdl, X, y, parameters):\n",
    "    clf = GridSearchCV(mdl, parameters, cv=3,n_jobs=-1,verbose=2)\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "grid_results_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Running this block takes atleast 16 hours on my laptop (I ran it over night and throughout the day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Model Type: Decision Tree-----\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:  2.9min remaining:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  3.2min remaining:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_criterion param_max_depth                                    params  split0_test_score  split1_test_score  split2_test_score  mean_test_score  std_test_score  rank_test_score\n",
      "0     184.355548      4.922018         2.710743        0.825086            gini            None  {'criterion': 'gini', 'max_depth': None}           0.762299           0.761282           0.761436         0.761673        0.000447                4\n",
      "1     155.908494      6.593873         5.021175        1.087320            gini              10    {'criterion': 'gini', 'max_depth': 10}           0.813291           0.813305           0.812691         0.813095        0.000286                1\n",
      "2     182.243585      5.141327         2.956693        1.084966            gini              25    {'criterion': 'gini', 'max_depth': 25}           0.787238           0.789307           0.787129         0.787892        0.001002                2\n",
      "3     186.369032      1.167061         2.575447        0.268828            gini              50    {'criterion': 'gini', 'max_depth': 50}           0.762273           0.760985           0.761919         0.761726        0.000543                3\n",
      "\n",
      "---- Model Type: Logistic Regression-----\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed: 11.9min remaining: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed: 20.8min remaining:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 21.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C param_max_iter param_penalty param_solver                                                              params  split0_test_score  split1_test_score  split2_test_score  mean_test_score  std_test_score  rank_test_score\n",
      "0     675.300604    180.070227         4.558817        0.404909   0.001           1000            l2        lbfgs  {'C': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}           0.805364           0.804513           0.804621         0.804833        0.000378                4\n",
      "1     794.481125    127.650286         3.434471        0.620300    0.01           1000            l2        lbfgs   {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}           0.805795           0.804898           0.805067         0.805253        0.000389                3\n",
      "2    1200.693407     58.087237         2.222346        0.391902     0.1           1000            l2        lbfgs    {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}           0.805824           0.804970           0.805107         0.805300        0.000375                2\n",
      "3    1129.822945    213.614867         3.222764        2.346777       1           1000            l2        lbfgs      {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}           0.805844           0.804970           0.805112         0.805309        0.000383                1\n",
      "\n",
      "---- Model Type: KNN-----\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed: 14.7min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 16.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_n_neighbors               params  split0_test_score  split1_test_score  split2_test_score  mean_test_score  std_test_score  rank_test_score\n",
      "0      21.853885      1.359578       609.430161       31.576058                 5   {'n_neighbors': 5}           0.751766           0.752556           0.754015         0.752779        0.000931                3\n",
      "1      22.936045      1.017744       805.589381       33.495494                10  {'n_neighbors': 10}           0.763118           0.763711           0.764619         0.763816        0.000617                2\n",
      "2      22.747793      1.039565       918.214784       37.235375                15  {'n_neighbors': 15}           0.766609           0.766238           0.766857         0.766568        0.000255                1\n",
      "\n",
      "---- Model Type: Neural network-----\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 277.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_activation param_alpha param_hidden_layer_sizes param_learning_rate param_max_iter param_solver                                                                                                                                       params  split0_test_score  split1_test_score  split2_test_score  mean_test_score  std_test_score  rank_test_score\n",
      "0     4858.679597   2389.784801        10.535156        2.255036             tanh      0.0001             (10, 20, 10)            constant            200          sgd   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.816040           0.815357           0.814503         0.815300        0.000628               25\n",
      "1     1705.776983    506.340780         8.459095        1.524864             tanh      0.0001             (10, 20, 10)            constant            200         adam  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.817304           0.815614           0.815582         0.816167        0.000804                5\n",
      "2     5652.081582   4599.899240        12.127898        2.352047             tanh      0.0001             (10, 20, 10)            adaptive            200          sgd   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816616           0.816125           0.814338         0.815693        0.000979               13\n",
      "3     1770.401493    429.128994         9.235300        3.103334             tanh      0.0001             (10, 20, 10)            adaptive            200         adam  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.816716           0.815928           0.815360         0.816001        0.000556                7\n",
      "4     2865.522179   2725.490913         7.021217        1.099631             tanh      0.0001             (10, 10, 10)            constant            200          sgd   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.815711           0.814415           0.814309         0.814812        0.000638               34\n",
      "5      563.512311     62.478963         7.382921        1.160036             tanh      0.0001             (10, 10, 10)            constant            200         adam  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815985           0.815916           0.814361         0.815421        0.000750               20\n",
      "6     1182.655507    166.155432        39.372203       42.443071             tanh      0.0001             (10, 10, 10)            adaptive            200          sgd   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.815024           0.816028           0.814863         0.815305        0.000515               24\n",
      "7      457.382071    132.299487         9.235632        1.580995             tanh      0.0001             (10, 10, 10)            adaptive            200         adam  {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.816454           0.814706           0.815405         0.815522        0.000718               18\n",
      "8      973.243855    263.382945        11.114275        0.610104             tanh      0.0001              (20, 10, 5)            constant            200          sgd    {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.816248           0.815306           0.815177         0.815577        0.000477               16\n",
      "9      695.238071    243.170400        11.845321        3.104368             tanh      0.0001              (20, 10, 5)            constant            200         adam   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815574           0.815680           0.815537         0.815597        0.000060               15\n",
      "10    2731.500451   1331.713337        11.503237        1.588286             tanh      0.0001              (20, 10, 5)            adaptive            200          sgd    {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816351           0.816242           0.814855         0.815816        0.000681                9\n",
      "11     590.927460     20.261179        11.075379        2.809910             tanh      0.0001              (20, 10, 5)            adaptive            200         adam   {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.816168           0.816265           0.816262         0.816232        0.000045                4\n",
      "12     687.865969    104.193817        15.314705        1.738127             tanh        0.05             (10, 20, 10)            constant            200          sgd     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.814670           0.813687           0.813245         0.813867        0.000595               45\n",
      "13     488.891799    123.678928        11.397186        1.059759             tanh        0.05             (10, 20, 10)            constant            200         adam    {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.814567           0.813787           0.813230         0.813861        0.000548               46\n",
      "14    1400.469313    435.746521        14.370235        1.647869             tanh        0.05             (10, 20, 10)            adaptive            200          sgd     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.814855           0.813530           0.814521         0.814302        0.000563               39\n",
      "15     418.025512     31.816729        12.621914        6.385274             tanh        0.05             (10, 20, 10)            adaptive            200         adam    {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.814344           0.814601           0.813527         0.814157        0.000458               43\n",
      "16     649.991112     31.648965        23.848551       18.161545             tanh        0.05             (10, 10, 10)            constant            200          sgd     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.814952           0.813927           0.813102         0.813994        0.000757               44\n",
      "17     332.474468     28.579037        15.408457        7.169642             tanh        0.05             (10, 10, 10)            constant            200         adam    {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.814393           0.814643           0.813953         0.814330        0.000286               38\n",
      "18    1405.288158    508.467277        10.361623        3.755593             tanh        0.05             (10, 10, 10)            adaptive            200          sgd     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816254           0.815485           0.814001         0.815247        0.000935               26\n",
      "19     338.747025     19.968016        21.628488       14.456385             tanh        0.05             (10, 10, 10)            adaptive            200         adam    {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.814324           0.813516           0.813402         0.813747        0.000411               47\n",
      "20     845.524207     34.541927        12.075704        3.455794             tanh        0.05              (20, 10, 5)            constant            200          sgd      {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.815911           0.815856           0.814720         0.815496        0.000549               19\n",
      "21     449.946784     77.373868        13.318381        1.543003             tanh        0.05              (20, 10, 5)            constant            200         adam     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815557           0.815043           0.814104         0.814901        0.000602               32\n",
      "22    3518.514360   3110.004156        76.578859       85.488031             tanh        0.05              (20, 10, 5)            adaptive            200          sgd      {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816553           0.815996           0.815680         0.816076        0.000361                6\n",
      "23     393.554773     62.482970        10.721323        0.235446             tanh        0.05              (20, 10, 5)            adaptive            200         adam     {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.815380           0.815248           0.814252         0.814960        0.000504               31\n",
      "24    2367.151326   1603.611181        12.289465        2.792697             relu      0.0001             (10, 20, 10)            constant            200          sgd   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.816388           0.815739           0.814118         0.815415        0.000955               21\n",
      "25     942.354412    133.898264        17.165098        6.766450             relu      0.0001             (10, 20, 10)            constant            200         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.816080           0.815534           0.815585         0.815733        0.000246               12\n",
      "26    1594.385232    274.278866        12.275835        2.342944             relu      0.0001             (10, 20, 10)            adaptive            200          sgd   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.814170           0.814552           0.815154         0.814625        0.000405               35\n",
      "27     714.528360     96.085852        15.337644        4.074821             relu      0.0001             (10, 20, 10)            adaptive            200         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.816805           0.814235           0.814883         0.815308        0.001091               23\n",
      "28     897.658227     67.303606        19.699648       11.446405             relu      0.0001             (10, 10, 10)            constant            200          sgd   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.815038           0.814581           0.813895         0.814505        0.000469               37\n",
      "29     524.363931     10.428446        15.042102        4.667702             relu      0.0001             (10, 10, 10)            constant            200         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815951           0.815420           0.815488         0.815620        0.000236               14\n",
      "30    2251.319138    553.695176        15.390172        1.328915             relu      0.0001             (10, 10, 10)            adaptive            200          sgd   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816257           0.814426           0.814346         0.815010        0.000882               30\n",
      "31     668.433618    126.924963        15.876205        4.567634             relu      0.0001             (10, 10, 10)            adaptive            200         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.816131           0.815425           0.814543         0.815367        0.000649               22\n",
      "32    1276.103718    370.783189        14.415115        6.329537             relu      0.0001              (20, 10, 5)            constant            200          sgd    {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.816796           0.815825           0.815123         0.815915        0.000686                8\n",
      "33     606.667849    109.604952        14.992902        4.372323             relu      0.0001              (20, 10, 5)            constant            200         adam   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.816945           0.816607           0.815794         0.816448        0.000483                3\n",
      "34    3732.822687   1127.085391       166.469114      222.305128             relu      0.0001              (20, 10, 5)            adaptive            200          sgd    {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816408           0.814789           0.816119         0.815772        0.000705               10\n",
      "35     634.056428     94.478231         9.596271        2.246456             relu      0.0001              (20, 10, 5)            adaptive            200         adam   {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.817287           0.816627           0.815637         0.816517        0.000678                1\n",
      "36    2135.895914   1573.841906        18.672059        1.632022             relu        0.05             (10, 20, 10)            constant            200          sgd     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.815323           0.815554           0.814424         0.815100        0.000488               28\n",
      "37     586.205210     74.432852        10.678108        1.684355             relu        0.05             (10, 20, 10)            constant            200         adam    {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.814316           0.814832           0.813744         0.814297        0.000444               40\n",
      "38    1837.373328    544.625419        22.097565        3.733149             relu        0.05             (10, 20, 10)            adaptive            200          sgd     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816593           0.814983           0.815106         0.815561        0.000732               17\n",
      "39     456.563926     29.429714        19.977569        8.708256             relu        0.05             (10, 20, 10)            adaptive            200         adam    {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 20, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.814313           0.814264           0.813907         0.814161        0.000181               42\n",
      "40    1422.333297    916.279693        13.957005        6.517321             relu        0.05             (10, 10, 10)            constant            200          sgd     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.814658           0.814589           0.814315         0.814521        0.000148               36\n",
      "41     553.476734    175.986593        19.374516        3.034990             relu        0.05             (10, 10, 10)            constant            200         adam    {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815212           0.814044           0.813530         0.814262        0.000704               41\n",
      "42    1647.401989    471.161345        13.806408        6.447020             relu        0.05             (10, 10, 10)            adaptive            200          sgd     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.815506           0.815234           0.814321         0.815020        0.000507               29\n",
      "43     534.576681     49.613664        13.396837        3.458563             relu        0.05             (10, 10, 10)            adaptive            200         adam    {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.815095           0.812363           0.813621         0.813693        0.001117               48\n",
      "44    1320.286744    272.567647         5.805809        2.879224             relu        0.05              (20, 10, 5)            constant            200          sgd      {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'sgd'}           0.816388           0.816093           0.814806         0.815762        0.000687               11\n",
      "45     651.307296    273.767371        12.130892        4.357834             relu        0.05              (20, 10, 5)            constant            200         adam     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'adam'}           0.815061           0.815057           0.814344         0.814820        0.000337               33\n",
      "46    1101.108102     71.468389         3.295280        0.628847             relu        0.05              (20, 10, 5)            adaptive            200          sgd      {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'sgd'}           0.816491           0.816347           0.816539         0.816459        0.000081                2\n",
      "47     497.337952    103.748374         5.195769        0.519416             relu        0.05              (20, 10, 5)            adaptive            200         adam     {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20, 10, 5), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}           0.815683           0.814792           0.814872         0.815115        0.000403               27\n"
     ]
    }
   ],
   "source": [
    "#GRID SEARCH step - Naive Bayes skipped\n",
    "models_params_grid = {\"Decision Tree\":{'criterion':['gini'],'max_depth':[None,10,25,50]}, #Using gini since its faster and not much of a difference in scores\n",
    "                      \"Logistic Regression\":{'penalty':['l2'],'solver':['lbfgs'], 'C': [0.001,0.01,0.1,1],'max_iter':[1000]},\n",
    "                     \"KNN\": {'n_neighbors':[5,10,15]},\n",
    "                      \"Neural network\": { 'hidden_layer_sizes': [(10,20,10), (10,10,10), (20,10,5)],\n",
    "                                           'activation': ['tanh', 'relu'],\n",
    "                                           'solver': ['sgd', 'adam'],\n",
    "                                           'alpha': [0.0001, 0.05],\n",
    "                                           'learning_rate': ['constant','adaptive'],\n",
    "                                           'max_iter':[200]},\n",
    "                      #\"SVM\": {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "                     }\n",
    "        \n",
    "\n",
    "for mdl_type,params in models_params_grid.items():\n",
    "    print(f\"\\n---- Model Type: {mdl_type}-----\")\n",
    "    mdl = get_models(mdl_type)\n",
    "    if mdl_type in ['KNN']:\n",
    "        clf = run_models_w_grid_search(mdl, X_train_val_small, y_train_val, params)\n",
    "    else:\n",
    "        clf = run_models_w_grid_search(mdl, X_train_val, y_train_val, params)\n",
    "    grid_results_dfs[mdl_type] = pd.DataFrame(clf.cv_results_)\n",
    "    print(grid_results_dfs[mdl_type].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write python dict to a file. DON'T RUN UNLESS FULL GRID SEARCH WAS RUN\n",
    "if grid_results_dfs != {}:\n",
    "    output = open('myfileV2.pkl', 'wb')\n",
    "    pkl.dump(grid_results_dfs, output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search results: A data frame is printed with scores for each model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Decision Tree------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184.355548</td>\n",
       "      <td>4.922018</td>\n",
       "      <td>2.710743</td>\n",
       "      <td>0.825086</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None}</td>\n",
       "      <td>0.762299</td>\n",
       "      <td>0.761282</td>\n",
       "      <td>0.761436</td>\n",
       "      <td>0.761673</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155.908494</td>\n",
       "      <td>6.593873</td>\n",
       "      <td>5.021175</td>\n",
       "      <td>1.087320</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10}</td>\n",
       "      <td>0.813291</td>\n",
       "      <td>0.813305</td>\n",
       "      <td>0.812691</td>\n",
       "      <td>0.813095</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182.243585</td>\n",
       "      <td>5.141327</td>\n",
       "      <td>2.956693</td>\n",
       "      <td>1.084966</td>\n",
       "      <td>gini</td>\n",
       "      <td>25</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 25}</td>\n",
       "      <td>0.787238</td>\n",
       "      <td>0.789307</td>\n",
       "      <td>0.787129</td>\n",
       "      <td>0.787892</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186.369032</td>\n",
       "      <td>1.167061</td>\n",
       "      <td>2.575447</td>\n",
       "      <td>0.268828</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50}</td>\n",
       "      <td>0.762273</td>\n",
       "      <td>0.760985</td>\n",
       "      <td>0.761919</td>\n",
       "      <td>0.761726</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     184.355548      4.922018         2.710743        0.825086   \n",
       "1     155.908494      6.593873         5.021175        1.087320   \n",
       "2     182.243585      5.141327         2.956693        1.084966   \n",
       "3     186.369032      1.167061         2.575447        0.268828   \n",
       "\n",
       "  param_criterion param_max_depth                                    params  \\\n",
       "0            gini            None  {'criterion': 'gini', 'max_depth': None}   \n",
       "1            gini              10    {'criterion': 'gini', 'max_depth': 10}   \n",
       "2            gini              25    {'criterion': 'gini', 'max_depth': 25}   \n",
       "3            gini              50    {'criterion': 'gini', 'max_depth': 50}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.762299           0.761282           0.761436         0.761673   \n",
       "1           0.813291           0.813305           0.812691         0.813095   \n",
       "2           0.787238           0.789307           0.787129         0.787892   \n",
       "3           0.762273           0.760985           0.761919         0.761726   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.000447                4  \n",
       "1        0.000286                1  \n",
       "2        0.001002                2  \n",
       "3        0.000543                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Logistic Regression------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675.300604</td>\n",
       "      <td>180.070227</td>\n",
       "      <td>4.558817</td>\n",
       "      <td>0.404909</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 0.001, 'max_iter': 1000, 'penalty': 'l2'...</td>\n",
       "      <td>0.805364</td>\n",
       "      <td>0.804513</td>\n",
       "      <td>0.804621</td>\n",
       "      <td>0.804833</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>794.481125</td>\n",
       "      <td>127.650286</td>\n",
       "      <td>3.434471</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2',...</td>\n",
       "      <td>0.805795</td>\n",
       "      <td>0.804898</td>\n",
       "      <td>0.805067</td>\n",
       "      <td>0.805253</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200.693407</td>\n",
       "      <td>58.087237</td>\n",
       "      <td>2.222346</td>\n",
       "      <td>0.391902</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', ...</td>\n",
       "      <td>0.805824</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>0.805107</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1129.822945</td>\n",
       "      <td>213.614867</td>\n",
       "      <td>3.222764</td>\n",
       "      <td>2.346777</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 1, 'max_iter': 1000, 'penalty': 'l2', 's...</td>\n",
       "      <td>0.805844</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>0.805112</td>\n",
       "      <td>0.805309</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0     675.300604    180.070227         4.558817        0.404909   0.001   \n",
       "1     794.481125    127.650286         3.434471        0.620300    0.01   \n",
       "2    1200.693407     58.087237         2.222346        0.391902     0.1   \n",
       "3    1129.822945    213.614867         3.222764        2.346777       1   \n",
       "\n",
       "  param_max_iter param_penalty param_solver  \\\n",
       "0           1000            l2        lbfgs   \n",
       "1           1000            l2        lbfgs   \n",
       "2           1000            l2        lbfgs   \n",
       "3           1000            l2        lbfgs   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'C': 0.001, 'max_iter': 1000, 'penalty': 'l2'...           0.805364   \n",
       "1  {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2',...           0.805795   \n",
       "2  {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', ...           0.805824   \n",
       "3  {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 's...           0.805844   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.804513           0.804621         0.804833        0.000378   \n",
       "1           0.804898           0.805067         0.805253        0.000389   \n",
       "2           0.804970           0.805107         0.805300        0.000375   \n",
       "3           0.804970           0.805112         0.805309        0.000383   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                3  \n",
       "2                2  \n",
       "3                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------KNN------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.853885</td>\n",
       "      <td>1.359578</td>\n",
       "      <td>609.430161</td>\n",
       "      <td>31.576058</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.751766</td>\n",
       "      <td>0.752556</td>\n",
       "      <td>0.754015</td>\n",
       "      <td>0.752779</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.936045</td>\n",
       "      <td>1.017744</td>\n",
       "      <td>805.589381</td>\n",
       "      <td>33.495494</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "      <td>0.763118</td>\n",
       "      <td>0.763711</td>\n",
       "      <td>0.764619</td>\n",
       "      <td>0.763816</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.747793</td>\n",
       "      <td>1.039565</td>\n",
       "      <td>918.214784</td>\n",
       "      <td>37.235375</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_neighbors': 15}</td>\n",
       "      <td>0.766609</td>\n",
       "      <td>0.766238</td>\n",
       "      <td>0.766857</td>\n",
       "      <td>0.766568</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      21.853885      1.359578       609.430161       31.576058   \n",
       "1      22.936045      1.017744       805.589381       33.495494   \n",
       "2      22.747793      1.039565       918.214784       37.235375   \n",
       "\n",
       "  param_n_neighbors               params  split0_test_score  \\\n",
       "0                 5   {'n_neighbors': 5}           0.751766   \n",
       "1                10  {'n_neighbors': 10}           0.763118   \n",
       "2                15  {'n_neighbors': 15}           0.766609   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.752556           0.754015         0.752779        0.000931   \n",
       "1           0.763711           0.764619         0.763816        0.000617   \n",
       "2           0.766238           0.766857         0.766568        0.000255   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                2  \n",
       "2                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Neural network------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4858.679597</td>\n",
       "      <td>2389.784801</td>\n",
       "      <td>10.535156</td>\n",
       "      <td>2.255036</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816040</td>\n",
       "      <td>0.815357</td>\n",
       "      <td>0.814503</td>\n",
       "      <td>0.815300</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1705.776983</td>\n",
       "      <td>506.340780</td>\n",
       "      <td>8.459095</td>\n",
       "      <td>1.524864</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.817304</td>\n",
       "      <td>0.815614</td>\n",
       "      <td>0.815582</td>\n",
       "      <td>0.816167</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5652.081582</td>\n",
       "      <td>4599.899240</td>\n",
       "      <td>12.127898</td>\n",
       "      <td>2.352047</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816616</td>\n",
       "      <td>0.816125</td>\n",
       "      <td>0.814338</td>\n",
       "      <td>0.815693</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1770.401493</td>\n",
       "      <td>429.128994</td>\n",
       "      <td>9.235300</td>\n",
       "      <td>3.103334</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816716</td>\n",
       "      <td>0.815928</td>\n",
       "      <td>0.815360</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2865.522179</td>\n",
       "      <td>2725.490913</td>\n",
       "      <td>7.021217</td>\n",
       "      <td>1.099631</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815711</td>\n",
       "      <td>0.814415</td>\n",
       "      <td>0.814309</td>\n",
       "      <td>0.814812</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>563.512311</td>\n",
       "      <td>62.478963</td>\n",
       "      <td>7.382921</td>\n",
       "      <td>1.160036</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815985</td>\n",
       "      <td>0.815916</td>\n",
       "      <td>0.814361</td>\n",
       "      <td>0.815421</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1182.655507</td>\n",
       "      <td>166.155432</td>\n",
       "      <td>39.372203</td>\n",
       "      <td>42.443071</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815024</td>\n",
       "      <td>0.816028</td>\n",
       "      <td>0.814863</td>\n",
       "      <td>0.815305</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>457.382071</td>\n",
       "      <td>132.299487</td>\n",
       "      <td>9.235632</td>\n",
       "      <td>1.580995</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.815405</td>\n",
       "      <td>0.815522</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>973.243855</td>\n",
       "      <td>263.382945</td>\n",
       "      <td>11.114275</td>\n",
       "      <td>0.610104</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>0.815306</td>\n",
       "      <td>0.815177</td>\n",
       "      <td>0.815577</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>695.238071</td>\n",
       "      <td>243.170400</td>\n",
       "      <td>11.845321</td>\n",
       "      <td>3.104368</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815574</td>\n",
       "      <td>0.815680</td>\n",
       "      <td>0.815537</td>\n",
       "      <td>0.815597</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2731.500451</td>\n",
       "      <td>1331.713337</td>\n",
       "      <td>11.503237</td>\n",
       "      <td>1.588286</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816351</td>\n",
       "      <td>0.816242</td>\n",
       "      <td>0.814855</td>\n",
       "      <td>0.815816</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>590.927460</td>\n",
       "      <td>20.261179</td>\n",
       "      <td>11.075379</td>\n",
       "      <td>2.809910</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816168</td>\n",
       "      <td>0.816265</td>\n",
       "      <td>0.816262</td>\n",
       "      <td>0.816232</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>687.865969</td>\n",
       "      <td>104.193817</td>\n",
       "      <td>15.314705</td>\n",
       "      <td>1.738127</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814670</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>0.813245</td>\n",
       "      <td>0.813867</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>488.891799</td>\n",
       "      <td>123.678928</td>\n",
       "      <td>11.397186</td>\n",
       "      <td>1.059759</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814567</td>\n",
       "      <td>0.813787</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.813861</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1400.469313</td>\n",
       "      <td>435.746521</td>\n",
       "      <td>14.370235</td>\n",
       "      <td>1.647869</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814855</td>\n",
       "      <td>0.813530</td>\n",
       "      <td>0.814521</td>\n",
       "      <td>0.814302</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>418.025512</td>\n",
       "      <td>31.816729</td>\n",
       "      <td>12.621914</td>\n",
       "      <td>6.385274</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814344</td>\n",
       "      <td>0.814601</td>\n",
       "      <td>0.813527</td>\n",
       "      <td>0.814157</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>649.991112</td>\n",
       "      <td>31.648965</td>\n",
       "      <td>23.848551</td>\n",
       "      <td>18.161545</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814952</td>\n",
       "      <td>0.813927</td>\n",
       "      <td>0.813102</td>\n",
       "      <td>0.813994</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>332.474468</td>\n",
       "      <td>28.579037</td>\n",
       "      <td>15.408457</td>\n",
       "      <td>7.169642</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>0.814643</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.814330</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1405.288158</td>\n",
       "      <td>508.467277</td>\n",
       "      <td>10.361623</td>\n",
       "      <td>3.755593</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.816254</td>\n",
       "      <td>0.815485</td>\n",
       "      <td>0.814001</td>\n",
       "      <td>0.815247</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>338.747025</td>\n",
       "      <td>19.968016</td>\n",
       "      <td>21.628488</td>\n",
       "      <td>14.456385</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814324</td>\n",
       "      <td>0.813516</td>\n",
       "      <td>0.813402</td>\n",
       "      <td>0.813747</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>845.524207</td>\n",
       "      <td>34.541927</td>\n",
       "      <td>12.075704</td>\n",
       "      <td>3.455794</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815911</td>\n",
       "      <td>0.815856</td>\n",
       "      <td>0.814720</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>449.946784</td>\n",
       "      <td>77.373868</td>\n",
       "      <td>13.318381</td>\n",
       "      <td>1.543003</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815557</td>\n",
       "      <td>0.815043</td>\n",
       "      <td>0.814104</td>\n",
       "      <td>0.814901</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3518.514360</td>\n",
       "      <td>3110.004156</td>\n",
       "      <td>76.578859</td>\n",
       "      <td>85.488031</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.816553</td>\n",
       "      <td>0.815996</td>\n",
       "      <td>0.815680</td>\n",
       "      <td>0.816076</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>393.554773</td>\n",
       "      <td>62.482970</td>\n",
       "      <td>10.721323</td>\n",
       "      <td>0.235446</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815380</td>\n",
       "      <td>0.815248</td>\n",
       "      <td>0.814252</td>\n",
       "      <td>0.814960</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2367.151326</td>\n",
       "      <td>1603.611181</td>\n",
       "      <td>12.289465</td>\n",
       "      <td>2.792697</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816388</td>\n",
       "      <td>0.815739</td>\n",
       "      <td>0.814118</td>\n",
       "      <td>0.815415</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>942.354412</td>\n",
       "      <td>133.898264</td>\n",
       "      <td>17.165098</td>\n",
       "      <td>6.766450</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816080</td>\n",
       "      <td>0.815534</td>\n",
       "      <td>0.815585</td>\n",
       "      <td>0.815733</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1594.385232</td>\n",
       "      <td>274.278866</td>\n",
       "      <td>12.275835</td>\n",
       "      <td>2.342944</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.814170</td>\n",
       "      <td>0.814552</td>\n",
       "      <td>0.815154</td>\n",
       "      <td>0.814625</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>714.528360</td>\n",
       "      <td>96.085852</td>\n",
       "      <td>15.337644</td>\n",
       "      <td>4.074821</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816805</td>\n",
       "      <td>0.814235</td>\n",
       "      <td>0.814883</td>\n",
       "      <td>0.815308</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>897.658227</td>\n",
       "      <td>67.303606</td>\n",
       "      <td>19.699648</td>\n",
       "      <td>11.446405</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815038</td>\n",
       "      <td>0.814581</td>\n",
       "      <td>0.813895</td>\n",
       "      <td>0.814505</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>524.363931</td>\n",
       "      <td>10.428446</td>\n",
       "      <td>15.042102</td>\n",
       "      <td>4.667702</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.815951</td>\n",
       "      <td>0.815420</td>\n",
       "      <td>0.815488</td>\n",
       "      <td>0.815620</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2251.319138</td>\n",
       "      <td>553.695176</td>\n",
       "      <td>15.390172</td>\n",
       "      <td>1.328915</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816257</td>\n",
       "      <td>0.814426</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.815010</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>668.433618</td>\n",
       "      <td>126.924963</td>\n",
       "      <td>15.876205</td>\n",
       "      <td>4.567634</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816131</td>\n",
       "      <td>0.815425</td>\n",
       "      <td>0.814543</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1276.103718</td>\n",
       "      <td>370.783189</td>\n",
       "      <td>14.415115</td>\n",
       "      <td>6.329537</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816796</td>\n",
       "      <td>0.815825</td>\n",
       "      <td>0.815123</td>\n",
       "      <td>0.815915</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>606.667849</td>\n",
       "      <td>109.604952</td>\n",
       "      <td>14.992902</td>\n",
       "      <td>4.372323</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816945</td>\n",
       "      <td>0.816607</td>\n",
       "      <td>0.815794</td>\n",
       "      <td>0.816448</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3732.822687</td>\n",
       "      <td>1127.085391</td>\n",
       "      <td>166.469114</td>\n",
       "      <td>222.305128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.816408</td>\n",
       "      <td>0.814789</td>\n",
       "      <td>0.816119</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>634.056428</td>\n",
       "      <td>94.478231</td>\n",
       "      <td>9.596271</td>\n",
       "      <td>2.246456</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.817287</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.815637</td>\n",
       "      <td>0.816517</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2135.895914</td>\n",
       "      <td>1573.841906</td>\n",
       "      <td>18.672059</td>\n",
       "      <td>1.632022</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815323</td>\n",
       "      <td>0.815554</td>\n",
       "      <td>0.814424</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>586.205210</td>\n",
       "      <td>74.432852</td>\n",
       "      <td>10.678108</td>\n",
       "      <td>1.684355</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814316</td>\n",
       "      <td>0.814832</td>\n",
       "      <td>0.813744</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1837.373328</td>\n",
       "      <td>544.625419</td>\n",
       "      <td>22.097565</td>\n",
       "      <td>3.733149</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.816593</td>\n",
       "      <td>0.814983</td>\n",
       "      <td>0.815106</td>\n",
       "      <td>0.815561</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>456.563926</td>\n",
       "      <td>29.429714</td>\n",
       "      <td>19.977569</td>\n",
       "      <td>8.708256</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 20, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>0.814264</td>\n",
       "      <td>0.813907</td>\n",
       "      <td>0.814161</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1422.333297</td>\n",
       "      <td>916.279693</td>\n",
       "      <td>13.957005</td>\n",
       "      <td>6.517321</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.814658</td>\n",
       "      <td>0.814589</td>\n",
       "      <td>0.814315</td>\n",
       "      <td>0.814521</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>553.476734</td>\n",
       "      <td>175.986593</td>\n",
       "      <td>19.374516</td>\n",
       "      <td>3.034990</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815212</td>\n",
       "      <td>0.814044</td>\n",
       "      <td>0.813530</td>\n",
       "      <td>0.814262</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1647.401989</td>\n",
       "      <td>471.161345</td>\n",
       "      <td>13.806408</td>\n",
       "      <td>6.447020</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815506</td>\n",
       "      <td>0.815234</td>\n",
       "      <td>0.814321</td>\n",
       "      <td>0.815020</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>534.576681</td>\n",
       "      <td>49.613664</td>\n",
       "      <td>13.396837</td>\n",
       "      <td>3.458563</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815095</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.813621</td>\n",
       "      <td>0.813693</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1320.286744</td>\n",
       "      <td>272.567647</td>\n",
       "      <td>5.805809</td>\n",
       "      <td>2.879224</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.816388</td>\n",
       "      <td>0.816093</td>\n",
       "      <td>0.814806</td>\n",
       "      <td>0.815762</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>651.307296</td>\n",
       "      <td>273.767371</td>\n",
       "      <td>12.130892</td>\n",
       "      <td>4.357834</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>constant</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815061</td>\n",
       "      <td>0.815057</td>\n",
       "      <td>0.814344</td>\n",
       "      <td>0.814820</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1101.108102</td>\n",
       "      <td>71.468389</td>\n",
       "      <td>3.295280</td>\n",
       "      <td>0.628847</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.816491</td>\n",
       "      <td>0.816347</td>\n",
       "      <td>0.816539</td>\n",
       "      <td>0.816459</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>497.337952</td>\n",
       "      <td>103.748374</td>\n",
       "      <td>5.195769</td>\n",
       "      <td>0.519416</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.05</td>\n",
       "      <td>(20, 10, 5)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.05, 'hidden_...</td>\n",
       "      <td>0.815683</td>\n",
       "      <td>0.814792</td>\n",
       "      <td>0.814872</td>\n",
       "      <td>0.815115</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     4858.679597   2389.784801        10.535156        2.255036   \n",
       "1     1705.776983    506.340780         8.459095        1.524864   \n",
       "2     5652.081582   4599.899240        12.127898        2.352047   \n",
       "3     1770.401493    429.128994         9.235300        3.103334   \n",
       "4     2865.522179   2725.490913         7.021217        1.099631   \n",
       "5      563.512311     62.478963         7.382921        1.160036   \n",
       "6     1182.655507    166.155432        39.372203       42.443071   \n",
       "7      457.382071    132.299487         9.235632        1.580995   \n",
       "8      973.243855    263.382945        11.114275        0.610104   \n",
       "9      695.238071    243.170400        11.845321        3.104368   \n",
       "10    2731.500451   1331.713337        11.503237        1.588286   \n",
       "11     590.927460     20.261179        11.075379        2.809910   \n",
       "12     687.865969    104.193817        15.314705        1.738127   \n",
       "13     488.891799    123.678928        11.397186        1.059759   \n",
       "14    1400.469313    435.746521        14.370235        1.647869   \n",
       "15     418.025512     31.816729        12.621914        6.385274   \n",
       "16     649.991112     31.648965        23.848551       18.161545   \n",
       "17     332.474468     28.579037        15.408457        7.169642   \n",
       "18    1405.288158    508.467277        10.361623        3.755593   \n",
       "19     338.747025     19.968016        21.628488       14.456385   \n",
       "20     845.524207     34.541927        12.075704        3.455794   \n",
       "21     449.946784     77.373868        13.318381        1.543003   \n",
       "22    3518.514360   3110.004156        76.578859       85.488031   \n",
       "23     393.554773     62.482970        10.721323        0.235446   \n",
       "24    2367.151326   1603.611181        12.289465        2.792697   \n",
       "25     942.354412    133.898264        17.165098        6.766450   \n",
       "26    1594.385232    274.278866        12.275835        2.342944   \n",
       "27     714.528360     96.085852        15.337644        4.074821   \n",
       "28     897.658227     67.303606        19.699648       11.446405   \n",
       "29     524.363931     10.428446        15.042102        4.667702   \n",
       "30    2251.319138    553.695176        15.390172        1.328915   \n",
       "31     668.433618    126.924963        15.876205        4.567634   \n",
       "32    1276.103718    370.783189        14.415115        6.329537   \n",
       "33     606.667849    109.604952        14.992902        4.372323   \n",
       "34    3732.822687   1127.085391       166.469114      222.305128   \n",
       "35     634.056428     94.478231         9.596271        2.246456   \n",
       "36    2135.895914   1573.841906        18.672059        1.632022   \n",
       "37     586.205210     74.432852        10.678108        1.684355   \n",
       "38    1837.373328    544.625419        22.097565        3.733149   \n",
       "39     456.563926     29.429714        19.977569        8.708256   \n",
       "40    1422.333297    916.279693        13.957005        6.517321   \n",
       "41     553.476734    175.986593        19.374516        3.034990   \n",
       "42    1647.401989    471.161345        13.806408        6.447020   \n",
       "43     534.576681     49.613664        13.396837        3.458563   \n",
       "44    1320.286744    272.567647         5.805809        2.879224   \n",
       "45     651.307296    273.767371        12.130892        4.357834   \n",
       "46    1101.108102     71.468389         3.295280        0.628847   \n",
       "47     497.337952    103.748374         5.195769        0.519416   \n",
       "\n",
       "   param_activation param_alpha param_hidden_layer_sizes param_learning_rate  \\\n",
       "0              tanh      0.0001             (10, 20, 10)            constant   \n",
       "1              tanh      0.0001             (10, 20, 10)            constant   \n",
       "2              tanh      0.0001             (10, 20, 10)            adaptive   \n",
       "3              tanh      0.0001             (10, 20, 10)            adaptive   \n",
       "4              tanh      0.0001             (10, 10, 10)            constant   \n",
       "5              tanh      0.0001             (10, 10, 10)            constant   \n",
       "6              tanh      0.0001             (10, 10, 10)            adaptive   \n",
       "7              tanh      0.0001             (10, 10, 10)            adaptive   \n",
       "8              tanh      0.0001              (20, 10, 5)            constant   \n",
       "9              tanh      0.0001              (20, 10, 5)            constant   \n",
       "10             tanh      0.0001              (20, 10, 5)            adaptive   \n",
       "11             tanh      0.0001              (20, 10, 5)            adaptive   \n",
       "12             tanh        0.05             (10, 20, 10)            constant   \n",
       "13             tanh        0.05             (10, 20, 10)            constant   \n",
       "14             tanh        0.05             (10, 20, 10)            adaptive   \n",
       "15             tanh        0.05             (10, 20, 10)            adaptive   \n",
       "16             tanh        0.05             (10, 10, 10)            constant   \n",
       "17             tanh        0.05             (10, 10, 10)            constant   \n",
       "18             tanh        0.05             (10, 10, 10)            adaptive   \n",
       "19             tanh        0.05             (10, 10, 10)            adaptive   \n",
       "20             tanh        0.05              (20, 10, 5)            constant   \n",
       "21             tanh        0.05              (20, 10, 5)            constant   \n",
       "22             tanh        0.05              (20, 10, 5)            adaptive   \n",
       "23             tanh        0.05              (20, 10, 5)            adaptive   \n",
       "24             relu      0.0001             (10, 20, 10)            constant   \n",
       "25             relu      0.0001             (10, 20, 10)            constant   \n",
       "26             relu      0.0001             (10, 20, 10)            adaptive   \n",
       "27             relu      0.0001             (10, 20, 10)            adaptive   \n",
       "28             relu      0.0001             (10, 10, 10)            constant   \n",
       "29             relu      0.0001             (10, 10, 10)            constant   \n",
       "30             relu      0.0001             (10, 10, 10)            adaptive   \n",
       "31             relu      0.0001             (10, 10, 10)            adaptive   \n",
       "32             relu      0.0001              (20, 10, 5)            constant   \n",
       "33             relu      0.0001              (20, 10, 5)            constant   \n",
       "34             relu      0.0001              (20, 10, 5)            adaptive   \n",
       "35             relu      0.0001              (20, 10, 5)            adaptive   \n",
       "36             relu        0.05             (10, 20, 10)            constant   \n",
       "37             relu        0.05             (10, 20, 10)            constant   \n",
       "38             relu        0.05             (10, 20, 10)            adaptive   \n",
       "39             relu        0.05             (10, 20, 10)            adaptive   \n",
       "40             relu        0.05             (10, 10, 10)            constant   \n",
       "41             relu        0.05             (10, 10, 10)            constant   \n",
       "42             relu        0.05             (10, 10, 10)            adaptive   \n",
       "43             relu        0.05             (10, 10, 10)            adaptive   \n",
       "44             relu        0.05              (20, 10, 5)            constant   \n",
       "45             relu        0.05              (20, 10, 5)            constant   \n",
       "46             relu        0.05              (20, 10, 5)            adaptive   \n",
       "47             relu        0.05              (20, 10, 5)            adaptive   \n",
       "\n",
       "   param_max_iter param_solver  \\\n",
       "0             200          sgd   \n",
       "1             200         adam   \n",
       "2             200          sgd   \n",
       "3             200         adam   \n",
       "4             200          sgd   \n",
       "5             200         adam   \n",
       "6             200          sgd   \n",
       "7             200         adam   \n",
       "8             200          sgd   \n",
       "9             200         adam   \n",
       "10            200          sgd   \n",
       "11            200         adam   \n",
       "12            200          sgd   \n",
       "13            200         adam   \n",
       "14            200          sgd   \n",
       "15            200         adam   \n",
       "16            200          sgd   \n",
       "17            200         adam   \n",
       "18            200          sgd   \n",
       "19            200         adam   \n",
       "20            200          sgd   \n",
       "21            200         adam   \n",
       "22            200          sgd   \n",
       "23            200         adam   \n",
       "24            200          sgd   \n",
       "25            200         adam   \n",
       "26            200          sgd   \n",
       "27            200         adam   \n",
       "28            200          sgd   \n",
       "29            200         adam   \n",
       "30            200          sgd   \n",
       "31            200         adam   \n",
       "32            200          sgd   \n",
       "33            200         adam   \n",
       "34            200          sgd   \n",
       "35            200         adam   \n",
       "36            200          sgd   \n",
       "37            200         adam   \n",
       "38            200          sgd   \n",
       "39            200         adam   \n",
       "40            200          sgd   \n",
       "41            200         adam   \n",
       "42            200          sgd   \n",
       "43            200         adam   \n",
       "44            200          sgd   \n",
       "45            200         adam   \n",
       "46            200          sgd   \n",
       "47            200         adam   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816040   \n",
       "1   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.817304   \n",
       "2   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816616   \n",
       "3   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816716   \n",
       "4   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.815711   \n",
       "5   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.815985   \n",
       "6   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.815024   \n",
       "7   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816454   \n",
       "8   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816248   \n",
       "9   {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.815574   \n",
       "10  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816351   \n",
       "11  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...           0.816168   \n",
       "12  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814670   \n",
       "13  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814567   \n",
       "14  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814855   \n",
       "15  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814344   \n",
       "16  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814952   \n",
       "17  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814393   \n",
       "18  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.816254   \n",
       "19  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.814324   \n",
       "20  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.815911   \n",
       "21  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.815557   \n",
       "22  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.816553   \n",
       "23  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...           0.815380   \n",
       "24  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816388   \n",
       "25  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816080   \n",
       "26  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.814170   \n",
       "27  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816805   \n",
       "28  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.815038   \n",
       "29  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.815951   \n",
       "30  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816257   \n",
       "31  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816131   \n",
       "32  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816796   \n",
       "33  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816945   \n",
       "34  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.816408   \n",
       "35  {'activation': 'relu', 'alpha': 0.0001, 'hidde...           0.817287   \n",
       "36  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815323   \n",
       "37  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.814316   \n",
       "38  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.816593   \n",
       "39  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.814313   \n",
       "40  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.814658   \n",
       "41  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815212   \n",
       "42  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815506   \n",
       "43  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815095   \n",
       "44  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.816388   \n",
       "45  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815061   \n",
       "46  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.816491   \n",
       "47  {'activation': 'relu', 'alpha': 0.05, 'hidden_...           0.815683   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.815357           0.814503         0.815300        0.000628   \n",
       "1            0.815614           0.815582         0.816167        0.000804   \n",
       "2            0.816125           0.814338         0.815693        0.000979   \n",
       "3            0.815928           0.815360         0.816001        0.000556   \n",
       "4            0.814415           0.814309         0.814812        0.000638   \n",
       "5            0.815916           0.814361         0.815421        0.000750   \n",
       "6            0.816028           0.814863         0.815305        0.000515   \n",
       "7            0.814706           0.815405         0.815522        0.000718   \n",
       "8            0.815306           0.815177         0.815577        0.000477   \n",
       "9            0.815680           0.815537         0.815597        0.000060   \n",
       "10           0.816242           0.814855         0.815816        0.000681   \n",
       "11           0.816265           0.816262         0.816232        0.000045   \n",
       "12           0.813687           0.813245         0.813867        0.000595   \n",
       "13           0.813787           0.813230         0.813861        0.000548   \n",
       "14           0.813530           0.814521         0.814302        0.000563   \n",
       "15           0.814601           0.813527         0.814157        0.000458   \n",
       "16           0.813927           0.813102         0.813994        0.000757   \n",
       "17           0.814643           0.813953         0.814330        0.000286   \n",
       "18           0.815485           0.814001         0.815247        0.000935   \n",
       "19           0.813516           0.813402         0.813747        0.000411   \n",
       "20           0.815856           0.814720         0.815496        0.000549   \n",
       "21           0.815043           0.814104         0.814901        0.000602   \n",
       "22           0.815996           0.815680         0.816076        0.000361   \n",
       "23           0.815248           0.814252         0.814960        0.000504   \n",
       "24           0.815739           0.814118         0.815415        0.000955   \n",
       "25           0.815534           0.815585         0.815733        0.000246   \n",
       "26           0.814552           0.815154         0.814625        0.000405   \n",
       "27           0.814235           0.814883         0.815308        0.001091   \n",
       "28           0.814581           0.813895         0.814505        0.000469   \n",
       "29           0.815420           0.815488         0.815620        0.000236   \n",
       "30           0.814426           0.814346         0.815010        0.000882   \n",
       "31           0.815425           0.814543         0.815367        0.000649   \n",
       "32           0.815825           0.815123         0.815915        0.000686   \n",
       "33           0.816607           0.815794         0.816448        0.000483   \n",
       "34           0.814789           0.816119         0.815772        0.000705   \n",
       "35           0.816627           0.815637         0.816517        0.000678   \n",
       "36           0.815554           0.814424         0.815100        0.000488   \n",
       "37           0.814832           0.813744         0.814297        0.000444   \n",
       "38           0.814983           0.815106         0.815561        0.000732   \n",
       "39           0.814264           0.813907         0.814161        0.000181   \n",
       "40           0.814589           0.814315         0.814521        0.000148   \n",
       "41           0.814044           0.813530         0.814262        0.000704   \n",
       "42           0.815234           0.814321         0.815020        0.000507   \n",
       "43           0.812363           0.813621         0.813693        0.001117   \n",
       "44           0.816093           0.814806         0.815762        0.000687   \n",
       "45           0.815057           0.814344         0.814820        0.000337   \n",
       "46           0.816347           0.816539         0.816459        0.000081   \n",
       "47           0.814792           0.814872         0.815115        0.000403   \n",
       "\n",
       "    rank_test_score  \n",
       "0                25  \n",
       "1                 5  \n",
       "2                13  \n",
       "3                 7  \n",
       "4                34  \n",
       "5                20  \n",
       "6                24  \n",
       "7                18  \n",
       "8                16  \n",
       "9                15  \n",
       "10                9  \n",
       "11                4  \n",
       "12               45  \n",
       "13               46  \n",
       "14               39  \n",
       "15               43  \n",
       "16               44  \n",
       "17               38  \n",
       "18               26  \n",
       "19               47  \n",
       "20               19  \n",
       "21               32  \n",
       "22                6  \n",
       "23               31  \n",
       "24               21  \n",
       "25               12  \n",
       "26               35  \n",
       "27               23  \n",
       "28               37  \n",
       "29               14  \n",
       "30               30  \n",
       "31               22  \n",
       "32                8  \n",
       "33                3  \n",
       "34               10  \n",
       "35                1  \n",
       "36               28  \n",
       "37               40  \n",
       "38               17  \n",
       "39               42  \n",
       "40               36  \n",
       "41               41  \n",
       "42               29  \n",
       "43               48  \n",
       "44               11  \n",
       "45               33  \n",
       "46                2  \n",
       "47               27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read python dict back from the file\n",
    "pkl_file = open('myfileV2.pkl', 'rb')\n",
    "mydict2 = pkl.load(pkl_file)\n",
    "pkl_file.close()\n",
    "for k in grid_results_dfs:\n",
    "    print(f\"\\n------{k}------\")\n",
    "    display(grid_results_dfs[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models with final Parameters\n",
    "Run the models with the best parameters extracted from grid search.\n",
    "NOTE: Running this section takes atleast 2 hours on my laptop (I ran it over night). With KNN and NN begin the slowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evalute_models(mdl, X, y, print_avg_fold_scores=True):\n",
    "    \"\"\" Given a model and X,y data run(train/validate set) and print the 5 fold cross validation scores\n",
    "    :param mdl: model to run\n",
    "    :param X: features Data\n",
    "    :param y: class targets\n",
    "    :param print_avg_fold_scores: if true show the average score across the folds\n",
    "    \"\"\"\n",
    "    #USING STRATIFIED KFOLD\n",
    "    scores = cross_validate(mdl, X, y,n_jobs=-1, cv=5, scoring=['precision_weighted','recall_weighted', 'f1_weighted'], return_train_score=True, return_estimator=True) #USES stratfied KFOLD when cv number is set\n",
    "    scores = {k.replace('test_','validation_'):v for k,v in scores.items()}\n",
    "    scores_to_print = {k:v for k,v in scores.items() if k != 'estimator'}\n",
    "    print(\"\".join(f\"{k}: {np.mean(v)}\\n\" if print_avg_fold_scores else f\"{k}: {v}\\n\" for k,v in scores_to_print.items() ))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Model Type: Decision Tree-----\n",
      "fit_time: 69.2485957622528\n",
      "score_time: 5.682601690292358\n",
      "validation_precision_weighted: 0.8115778044980138\n",
      "train_precision_weighted: 0.8135643547701934\n",
      "validation_recall_weighted: 0.8131239941835215\n",
      "train_recall_weighted: 0.8139061132355729\n",
      "validation_f1_weighted: 0.7853610101962274\n",
      "train_f1_weighted: 0.786163344373581\n",
      "\n",
      "---- Model Type: Logistic Regression-----\n",
      "fit_time: 817.3432823657989\n",
      "score_time: 5.200206279754639\n",
      "validation_precision_weighted: 0.7866220061281488\n",
      "train_precision_weighted: 0.7875847509994018\n",
      "validation_recall_weighted: 0.8052219277625404\n",
      "train_recall_weighted: 0.8054588478199409\n",
      "validation_f1_weighted: 0.7725349918553412\n",
      "train_f1_weighted: 0.772775639445875\n",
      "\n",
      "---- Model Type: Naive Bayes-----\n",
      "fit_time: 24.918556213378906\n",
      "score_time: 9.064157962799072\n",
      "validation_precision_weighted: 0.7593961189206764\n",
      "train_precision_weighted: 0.759652108533639\n",
      "validation_recall_weighted: 0.7528209098298866\n",
      "train_recall_weighted: 0.7529700539147643\n",
      "validation_f1_weighted: 0.7451795938345148\n",
      "train_f1_weighted: 0.7453682282204647\n",
      "\n",
      "---- Model Type: KNN-----\n",
      "fit_time: 23.70360403060913\n",
      "score_time: 204.86197485923768\n",
      "validation_precision_weighted: 0.7438061845054184\n",
      "train_precision_weighted: 0.8224769088400716\n",
      "validation_recall_weighted: 0.7551387225425324\n",
      "train_recall_weighted: 0.827114837345411\n",
      "validation_f1_weighted: 0.7476094553036416\n",
      "train_f1_weighted: 0.8213556633576922\n",
      "\n",
      "---- Model Type: Neural network-----\n",
      "fit_time: 321.81650705337523\n",
      "score_time: 7.285314416885376\n",
      "validation_precision_weighted: 0.8077044668515734\n",
      "train_precision_weighted: 0.8106126164286227\n",
      "validation_recall_weighted: 0.8163609386822757\n",
      "train_recall_weighted: 0.8175843084713306\n",
      "validation_f1_weighted: 0.7900313887557174\n",
      "train_f1_weighted: 0.7912797392369362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final Results and training\n",
    "final_model_params = {\"Decision Tree\":{'criterion':'gini','max_depth':10},\n",
    "                      \"Logistic Regression\":{'n_jobs':-1,'penalty':'l2','solver':'lbfgs', 'C': 1, 'max_iter':1000},\n",
    "                      \"Naive Bayes\" : {},\n",
    "                      'KNN': {'n_jobs':-1,'n_neighbors':5},\n",
    "                      'Neural network': {'hidden_layer_sizes': (20,10,5),\n",
    "                                            'activation': 'relu',\n",
    "                                            'solver': 'adam',\n",
    "                                            'alpha': 0.001,\n",
    "                                            'learning_rate': 'adaptive',\n",
    "                                            'max_iter':1000},\n",
    "                      #\"SVM\": {'kernel':'linear', 'C':10}\n",
    "                      }\n",
    "final_scores = {}\n",
    "for mdl_type,params in final_model_params.items():\n",
    "    print(f\"---- Model Type: {mdl_type}-----\")\n",
    "    mdl = get_models(mdl_type, params)\n",
    "    if mdl_type in ['KNN']:\n",
    "        final_scores[mdl_type] = train_evalute_models(mdl, X_train_val_small, y_train_val)\n",
    "    else:\n",
    "        final_scores[mdl_type] = train_evalute_models(mdl, X_train_val, y_train_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision Tree': {'fit_time': array([70.54034209, 71.20556092, 69.78635716, 69.55497599, 65.15574265]),\n",
       "  'score_time': array([5.55613947, 5.85035348, 5.80746841, 5.85134983, 5.34769726]),\n",
       "  'estimator': (DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10)),\n",
       "  'validation_precision_weighted': array([0.81430637, 0.81206898, 0.81316412, 0.8065619 , 0.81178765]),\n",
       "  'train_precision_weighted': array([0.81394266, 0.81425473, 0.81496987, 0.80867906, 0.81597546]),\n",
       "  'validation_recall_weighted': array([0.81335306, 0.81335217, 0.81358053, 0.81300488, 0.81232933]),\n",
       "  'train_recall_weighted': array([0.81332839, 0.81421587, 0.81400178, 0.81390069, 0.81408385]),\n",
       "  'validation_f1_weighted': array([0.78592913, 0.78610272, 0.78499278, 0.78410185, 0.78567856]),\n",
       "  'train_f1_weighted': array([0.78587666, 0.78657164, 0.78547096, 0.78529736, 0.7876001 ])},\n",
       " 'Logistic Regression': {'fit_time': array([892.39099312, 824.14673638, 751.04224205, 799.85269952,\n",
       "         819.28374076]),\n",
       "  'score_time': array([4.15844584, 4.99164987, 5.84237838, 5.42548895, 5.58306837]),\n",
       "  'estimator': (LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1)),\n",
       "  'validation_precision_weighted': array([0.7918311 , 0.78623688, 0.78519695, 0.78427213, 0.78557296]),\n",
       "  'train_precision_weighted': array([0.78660114, 0.78700329, 0.78727433, 0.78907362, 0.78797138]),\n",
       "  'validation_recall_weighted': array([0.80614085, 0.80497436, 0.80491727, 0.80538825, 0.80468891]),\n",
       "  'train_recall_weighted': array([0.80517536, 0.80553002, 0.80556094, 0.80538135, 0.80564657]),\n",
       "  'validation_f1_weighted': array([0.77363968, 0.77251661, 0.77210785, 0.77224671, 0.77216411]),\n",
       "  'train_f1_weighted': array([0.77239238, 0.77275172, 0.77277204, 0.77270967, 0.77325238])},\n",
       " 'Naive Bayes': {'fit_time': array([24.56330585, 23.88312554, 24.11849475, 27.48549128, 24.54236364]),\n",
       "  'score_time': array([8.94308186, 9.3609643 , 9.00491619, 8.99195051, 9.01987696]),\n",
       "  'estimator': (BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB()),\n",
       "  'validation_precision_weighted': array([0.75851062, 0.75977883, 0.76096075, 0.75890594, 0.75882445]),\n",
       "  'train_precision_weighted': array([0.75946852, 0.75981128, 0.75982759, 0.75960923, 0.75954391]),\n",
       "  'validation_recall_weighted': array([0.75197789, 0.75289965, 0.75403191, 0.75244769, 0.75274741]),\n",
       "  'train_recall_weighted': array([0.75270578, 0.75305099, 0.75320679, 0.75296417, 0.75292254]),\n",
       "  'validation_f1_weighted': array([0.74476701, 0.74508527, 0.74614625, 0.74462439, 0.74527504]),\n",
       "  'train_f1_weighted': array([0.74551919, 0.74522438, 0.74525748, 0.74530215, 0.74553794])},\n",
       " 'KNN': {'fit_time': array([20.38846827, 22.55966425, 23.63279343, 21.17935634, 30.75773787]),\n",
       "  'score_time': array([201.53307724, 204.4313271 , 204.33259106, 202.30900264,\n",
       "         211.70387626]),\n",
       "  'estimator': (KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1)),\n",
       "  'validation_precision_weighted': array([0.74296275, 0.74354138, 0.74371996, 0.74377907, 0.74502775]),\n",
       "  'train_precision_weighted': array([0.82261198, 0.82257839, 0.82218945, 0.82234797, 0.82265674]),\n",
       "  'validation_recall_weighted': array([0.75399502, 0.75496912, 0.75532117, 0.75540205, 0.75600624]),\n",
       "  'train_recall_weighted': array([0.8271808 , 0.82741412, 0.82681825, 0.82701807, 0.82714295]),\n",
       "  'validation_f1_weighted': array([0.74676449, 0.74738432, 0.74749673, 0.74775915, 0.74864258]),\n",
       "  'train_f1_weighted': array([0.82148852, 0.82151588, 0.82106501, 0.82124323, 0.82146568])},\n",
       " 'Neural network': {'fit_time': array([372.11378956, 280.64840961, 266.73960924, 349.81442666,\n",
       "         339.7663002 ]),\n",
       "  'score_time': array([6.44476032, 8.49727273, 8.30279279, 6.56145191, 6.62029433]),\n",
       "  'estimator': (MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000)),\n",
       "  'validation_precision_weighted': array([0.8110894 , 0.80387158, 0.81111317, 0.80615542, 0.80629276]),\n",
       "  'train_precision_weighted': array([0.81177187, 0.80654299, 0.8152711 , 0.81017806, 0.80929906]),\n",
       "  'validation_recall_weighted': array([0.81705432, 0.8162542 , 0.81652537, 0.81642547, 0.81554534]),\n",
       "  'train_recall_weighted': array([0.81736268, 0.8178208 , 0.8175639 , 0.81789097, 0.81728321]),\n",
       "  'validation_f1_weighted': array([0.79096618, 0.78965069, 0.78732838, 0.789609  , 0.79260269]),\n",
       "  'train_f1_weighted': array([0.79109226, 0.79118154, 0.78820695, 0.79136455, 0.79455341])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test set on final models\n",
    "NOTE: Running this section takes atleast 3 hours on my laptop (I ran it over night). With KNN being the slowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evalute_models(mdls, X, y, print_avg_fold_scores=True):\n",
    "    \"\"\" Given a model and X,y data run(test set) and print the 5 fold cross test scores\n",
    "    :param mdl: model to run\n",
    "    :param X: features Data\n",
    "    :param y: class targets\n",
    "    :param print_avg_fold_scores: if true show the average score across the folds\n",
    "    \"\"\"\n",
    "    scores = {'test_precision_weighted': np.array([]),\n",
    "     'test_recall_weighted': np.array([]),\n",
    "     'test_f1-score_weighted': np.array([])}\n",
    "    for mdl in mdls:\n",
    "        mdl = mdl\n",
    "        y_pred = mdl.predict(X)\n",
    "        curr_scores = classification_report(y,y_pred,output_dict=True)['weighted avg']\n",
    "        curr_scores = {\"test_\"+k+\"_weighted\":v for k,v in curr_scores.items() if k !='support'}\n",
    "        for k,v in curr_scores.items():\n",
    "            scores[k] = np.append(scores[k],np.array([v]))\n",
    "    scores['test_f1_weighted'] = scores.pop('test_f1-score_weighted')\n",
    "    scores\n",
    "        \n",
    "    print(\"\".join(f\"{k}: {np.mean(v)}\\n\" if print_avg_fold_scores else f\"{k}: {v}\\n\" for k,v in scores.items() ))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "test_precision_weighted: 0.7795802665825413\n",
      "test_recall_weighted: 0.7950623838598354\n",
      "test_f1_weighted: 0.7702869676517952\n",
      "\n",
      "Logistic Regression\n",
      "test_precision_weighted: 0.7856034664008693\n",
      "test_recall_weighted: 0.8036925079424886\n",
      "test_f1_weighted: 0.7752798260834223\n",
      "\n",
      "Naive Bayes\n",
      "test_precision_weighted: 0.757038875143927\n",
      "test_recall_weighted: 0.7506735059129795\n",
      "test_f1_weighted: 0.7430454986983965\n",
      "\n",
      "KNN\n",
      "test_precision_weighted: 0.5703632579273628\n",
      "test_recall_weighted: 0.6074997645084219\n",
      "test_f1_weighted: 0.5874700940333236\n",
      "\n",
      "Neural network\n",
      "test_precision_weighted: 0.7966360822282684\n",
      "test_recall_weighted: 0.7843633592231347\n",
      "test_f1_weighted: 0.7698399189779415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mdl_type, mdl_info in final_scores.items():\n",
    "    print(mdl_type)\n",
    "    if mdl_type in ['KNN']:\n",
    "        curr_scores = test_evalute_models(mdl_info['estimator'], X_test_small, y_test)\n",
    "    else:\n",
    "        curr_scores = test_evalute_models(mdl_info['estimator'], X_test, y_test)\n",
    "        \n",
    "    final_scores[mdl_type].update(curr_scores)\n",
    "\n",
    "# write python dict to a file\n",
    "output = open('finalscoresmyfile.pkl', 'wb')\n",
    "pkl.dump(final_scores, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision Tree': {'fit_time': array([70.54034209, 71.20556092, 69.78635716, 69.55497599, 65.15574265]),\n",
       "  'score_time': array([5.55613947, 5.85035348, 5.80746841, 5.85134983, 5.34769726]),\n",
       "  'estimator': (DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10),\n",
       "   DecisionTreeClassifier(max_depth=10)),\n",
       "  'validation_precision_weighted': array([0.81430637, 0.81206898, 0.81316412, 0.8065619 , 0.81178765]),\n",
       "  'train_precision_weighted': array([0.81394266, 0.81425473, 0.81496987, 0.80867906, 0.81597546]),\n",
       "  'validation_recall_weighted': array([0.81335306, 0.81335217, 0.81358053, 0.81300488, 0.81232933]),\n",
       "  'train_recall_weighted': array([0.81332839, 0.81421587, 0.81400178, 0.81390069, 0.81408385]),\n",
       "  'validation_f1_weighted': array([0.78592913, 0.78610272, 0.78499278, 0.78410185, 0.78567856]),\n",
       "  'train_f1_weighted': array([0.78587666, 0.78657164, 0.78547096, 0.78529736, 0.7876001 ]),\n",
       "  'test_precision_weighted': array([0.78323701, 0.77553417, 0.77984834, 0.77982616, 0.77945566]),\n",
       "  'test_recall_weighted': array([0.79438588, 0.79631263, 0.79637257, 0.79348673, 0.7947541 ]),\n",
       "  'test_f1_weighted': array([0.77083992, 0.77099773, 0.77073784, 0.76803202, 0.77082733])},\n",
       " 'Logistic Regression': {'fit_time': array([892.39099312, 824.14673638, 751.04224205, 799.85269952,\n",
       "         819.28374076]),\n",
       "  'score_time': array([4.15844584, 4.99164987, 5.84237838, 5.42548895, 5.58306837]),\n",
       "  'estimator': (LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1),\n",
       "   LogisticRegression(C=1, max_iter=1000, n_jobs=-1)),\n",
       "  'validation_precision_weighted': array([0.7918311 , 0.78623688, 0.78519695, 0.78427213, 0.78557296]),\n",
       "  'train_precision_weighted': array([0.78660114, 0.78700329, 0.78727433, 0.78907362, 0.78797138]),\n",
       "  'validation_recall_weighted': array([0.80614085, 0.80497436, 0.80491727, 0.80538825, 0.80468891]),\n",
       "  'train_recall_weighted': array([0.80517536, 0.80553002, 0.80556094, 0.80538135, 0.80564657]),\n",
       "  'validation_f1_weighted': array([0.77363968, 0.77251661, 0.77210785, 0.77224671, 0.77216411]),\n",
       "  'train_f1_weighted': array([0.77239238, 0.77275172, 0.77277204, 0.77270967, 0.77325238]),\n",
       "  'test_precision_weighted': array([0.78562352, 0.78584411, 0.78598031, 0.78529799, 0.78527141]),\n",
       "  'test_recall_weighted': array([0.80377985, 0.80376273, 0.80383123, 0.80355721, 0.80353152]),\n",
       "  'test_f1_weighted': array([0.77510787, 0.77551745, 0.77546055, 0.77489129, 0.77542198])},\n",
       " 'Naive Bayes': {'fit_time': array([24.56330585, 23.88312554, 24.11849475, 27.48549128, 24.54236364]),\n",
       "  'score_time': array([8.94308186, 9.3609643 , 9.00491619, 8.99195051, 9.01987696]),\n",
       "  'estimator': (BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB(),\n",
       "   BernoulliNB()),\n",
       "  'validation_precision_weighted': array([0.75851062, 0.75977883, 0.76096075, 0.75890594, 0.75882445]),\n",
       "  'train_precision_weighted': array([0.75946852, 0.75981128, 0.75982759, 0.75960923, 0.75954391]),\n",
       "  'validation_recall_weighted': array([0.75197789, 0.75289965, 0.75403191, 0.75244769, 0.75274741]),\n",
       "  'train_recall_weighted': array([0.75270578, 0.75305099, 0.75320679, 0.75296417, 0.75292254]),\n",
       "  'validation_f1_weighted': array([0.74476701, 0.74508527, 0.74614625, 0.74462439, 0.74527504]),\n",
       "  'train_f1_weighted': array([0.74551919, 0.74522438, 0.74525748, 0.74530215, 0.74553794]),\n",
       "  'test_precision_weighted': array([0.75690388, 0.75708004, 0.75714786, 0.75721718, 0.75684541]),\n",
       "  'test_recall_weighted': array([0.7503618 , 0.75067008, 0.75092698, 0.75075571, 0.75065295]),\n",
       "  'test_f1_weighted': array([0.7431312 , 0.74278668, 0.74294751, 0.74320519, 0.74315691])},\n",
       " 'KNN': {'fit_time': array([20.38846827, 22.55966425, 23.63279343, 21.17935634, 30.75773787]),\n",
       "  'score_time': array([201.53307724, 204.4313271 , 204.33259106, 202.30900264,\n",
       "         211.70387626]),\n",
       "  'estimator': (KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1),\n",
       "   KNeighborsClassifier(n_jobs=-1)),\n",
       "  'validation_precision_weighted': array([0.74296275, 0.74354138, 0.74371996, 0.74377907, 0.74502775]),\n",
       "  'train_precision_weighted': array([0.82261198, 0.82257839, 0.82218945, 0.82234797, 0.82265674]),\n",
       "  'validation_recall_weighted': array([0.75399502, 0.75496912, 0.75532117, 0.75540205, 0.75600624]),\n",
       "  'train_recall_weighted': array([0.8271808 , 0.82741412, 0.82681825, 0.82701807, 0.82714295]),\n",
       "  'validation_f1_weighted': array([0.74676449, 0.74738432, 0.74749673, 0.74775915, 0.74864258]),\n",
       "  'train_f1_weighted': array([0.82148852, 0.82151588, 0.82106501, 0.82124323, 0.82146568]),\n",
       "  'test_precision_weighted': array([0.56996319, 0.57072897, 0.57038463, 0.5698097 , 0.5709298 ]),\n",
       "  'test_recall_weighted': array([0.60958065, 0.6102229 , 0.60833041, 0.6045026 , 0.60486226]),\n",
       "  'test_f1_weighted': array([0.58812232, 0.58882384, 0.58787234, 0.58585278, 0.58667919])},\n",
       " 'Neural network': {'fit_time': array([372.11378956, 280.64840961, 266.73960924, 349.81442666,\n",
       "         339.7663002 ]),\n",
       "  'score_time': array([6.44476032, 8.49727273, 8.30279279, 6.56145191, 6.62029433]),\n",
       "  'estimator': (MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000),\n",
       "   MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 10, 5),\n",
       "                 learning_rate='adaptive', max_iter=1000)),\n",
       "  'validation_precision_weighted': array([0.8110894 , 0.80387158, 0.81111317, 0.80615542, 0.80629276]),\n",
       "  'train_precision_weighted': array([0.81177187, 0.80654299, 0.8152711 , 0.81017806, 0.80929906]),\n",
       "  'validation_recall_weighted': array([0.81705432, 0.8162542 , 0.81652537, 0.81642547, 0.81554534]),\n",
       "  'train_recall_weighted': array([0.81736268, 0.8178208 , 0.8175639 , 0.81789097, 0.81728321]),\n",
       "  'validation_f1_weighted': array([0.79096618, 0.78965069, 0.78732838, 0.789609  , 0.79260269]),\n",
       "  'train_f1_weighted': array([0.79109226, 0.79118154, 0.78820695, 0.79136455, 0.79455341]),\n",
       "  'test_precision_weighted': array([0.80129111, 0.79057722, 0.80094436, 0.79497207, 0.79539564]),\n",
       "  'test_recall_weighted': array([0.78584824, 0.78514605, 0.78529162, 0.78482064, 0.78071024]),\n",
       "  'test_f1_weighted': array([0.76967186, 0.77025557, 0.76480836, 0.77268472, 0.77177909])}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert scores to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert scores to dataframe\n",
    "scores_df = pd.DataFrame({k:dict([(k2,v2) for k2,v2 in v.items() if k2 != 'estimator']) for k,v in final_scores.items() }).T\n",
    "scores_df['merge_col'] = 1  \n",
    "temp_cv_df = pd.DataFrame([1,2,3,4,5],columns=['cv'])\n",
    "temp_cv_df['merge_col'] = 1  \n",
    "scores_df = scores_df.reset_index().rename(columns={'index':'model'}).merge(temp_cv_df,on='merge_col').drop(columns=['merge_col'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_time = ['fit_time', 'score_time']\n",
    "train_cols_with_scores = [\n",
    " 'train_f1_weighted',\n",
    " 'train_precision_weighted',\n",
    " 'train_recall_weighted']\n",
    "validation_cols_with_scores = [\n",
    " 'validation_f1_weighted',\n",
    " 'validation_precision_weighted',\n",
    " 'validation_recall_weighted']\n",
    "test_cols_with_scores = [\n",
    " 'test_f1_weighted',\n",
    " 'test_precision_weighted',\n",
    " 'test_recall_weighted']\n",
    "for c in train_cols_with_scores + validation_cols_with_scores +test_cols_with_scores+ cols_with_time:\n",
    "    scores_df.loc[:,c] = scores_df.apply(lambda row:row[c][row['cv']-1],axis=1)\n",
    "    \n",
    "scores_df = scores_df[['model', 'cv'] + train_cols_with_scores + validation_cols_with_scores + test_cols_with_scores + cols_with_time]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Neural network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_f1_weighted</th>\n",
       "      <td>0.786163</td>\n",
       "      <td>0.821356</td>\n",
       "      <td>0.772776</td>\n",
       "      <td>0.745368</td>\n",
       "      <td>0.791280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision_weighted</th>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.822477</td>\n",
       "      <td>0.787585</td>\n",
       "      <td>0.759652</td>\n",
       "      <td>0.810613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall_weighted</th>\n",
       "      <td>0.813906</td>\n",
       "      <td>0.827115</td>\n",
       "      <td>0.805459</td>\n",
       "      <td>0.752970</td>\n",
       "      <td>0.817584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_f1_weighted</th>\n",
       "      <td>0.785361</td>\n",
       "      <td>0.747609</td>\n",
       "      <td>0.772535</td>\n",
       "      <td>0.745180</td>\n",
       "      <td>0.790031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_precision_weighted</th>\n",
       "      <td>0.811578</td>\n",
       "      <td>0.743806</td>\n",
       "      <td>0.786622</td>\n",
       "      <td>0.759396</td>\n",
       "      <td>0.807704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_recall_weighted</th>\n",
       "      <td>0.813124</td>\n",
       "      <td>0.755139</td>\n",
       "      <td>0.805222</td>\n",
       "      <td>0.752821</td>\n",
       "      <td>0.816361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <td>0.770287</td>\n",
       "      <td>0.587470</td>\n",
       "      <td>0.775280</td>\n",
       "      <td>0.743045</td>\n",
       "      <td>0.769840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision_weighted</th>\n",
       "      <td>0.779580</td>\n",
       "      <td>0.570363</td>\n",
       "      <td>0.785603</td>\n",
       "      <td>0.757039</td>\n",
       "      <td>0.796636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall_weighted</th>\n",
       "      <td>0.795062</td>\n",
       "      <td>0.607500</td>\n",
       "      <td>0.803693</td>\n",
       "      <td>0.750674</td>\n",
       "      <td>0.784363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>69.248596</td>\n",
       "      <td>23.703604</td>\n",
       "      <td>817.343282</td>\n",
       "      <td>24.918556</td>\n",
       "      <td>321.816507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>5.682602</td>\n",
       "      <td>204.861975</td>\n",
       "      <td>5.200206</td>\n",
       "      <td>9.064158</td>\n",
       "      <td>7.285314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                          Decision Tree         KNN  Logistic Regression  \\\n",
       "train_f1_weighted                   0.786163    0.821356             0.772776   \n",
       "train_precision_weighted            0.813564    0.822477             0.787585   \n",
       "train_recall_weighted               0.813906    0.827115             0.805459   \n",
       "validation_f1_weighted              0.785361    0.747609             0.772535   \n",
       "validation_precision_weighted       0.811578    0.743806             0.786622   \n",
       "validation_recall_weighted          0.813124    0.755139             0.805222   \n",
       "test_f1_weighted                    0.770287    0.587470             0.775280   \n",
       "test_precision_weighted             0.779580    0.570363             0.785603   \n",
       "test_recall_weighted                0.795062    0.607500             0.803693   \n",
       "fit_time                           69.248596   23.703604           817.343282   \n",
       "score_time                          5.682602  204.861975             5.200206   \n",
       "\n",
       "model                          Naive Bayes  Neural network  \n",
       "train_f1_weighted                 0.745368        0.791280  \n",
       "train_precision_weighted          0.759652        0.810613  \n",
       "train_recall_weighted             0.752970        0.817584  \n",
       "validation_f1_weighted            0.745180        0.790031  \n",
       "validation_precision_weighted     0.759396        0.807704  \n",
       "validation_recall_weighted        0.752821        0.816361  \n",
       "test_f1_weighted                  0.743045        0.769840  \n",
       "test_precision_weighted           0.757039        0.796636  \n",
       "test_recall_weighted              0.750674        0.784363  \n",
       "fit_time                         24.918556      321.816507  \n",
       "score_time                        9.064158        7.285314  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.groupby(['model']).mean().drop(columns=['cv']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training scores across 5 folds')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "df_plot = scores_df.drop(columns=['cv']).melt(id_vars='model', value_vars=train_cols_with_scores)\n",
    "sns.boxplot(x='model', y='value', hue='variable', data=df_plot, ax=ax)\n",
    "ax.yaxis.grid(True) # Show the horizontal gridlines\n",
    "ax.xaxis.grid(True) # Show the vertical gridlines\n",
    "ax.set_ylim([0.74,0.84])  \n",
    "ax.set_title('Training scores across 5 folds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation scores across 5 folds')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "df_plot = scores_df.drop(columns=['cv']).melt(id_vars='model', value_vars=validation_cols_with_scores)\n",
    "sns.boxplot(x='model', y='value', hue='variable', data=df_plot, ax=ax)\n",
    "ax.yaxis.grid(True) # Show the horizontal gridlines\n",
    "ax.xaxis.grid(True) # Show the vertical gridlines\n",
    "ax.set_ylim([0.74,0.84])  \n",
    "ax.set_title('Validation scores across 5 folds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test scores across 5 folds')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "df_plot = scores_df.drop(columns=['cv']).melt(id_vars='model', value_vars=test_cols_with_scores)\n",
    "sns.boxplot(x='model', y='value', hue='variable', data=df_plot, ax=ax)\n",
    "ax.yaxis.grid(True) # Show the horizontal gridlines\n",
    "ax.xaxis.grid(True) # Show the vertical gridlines\n",
    "ax.set_ylim([0.55,0.85])  \n",
    "ax.set_title('Test scores across 5 folds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
